{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e15d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONAI Dual-Task (Segmentation + Classification) training using nnU-Net-style pipeline\n",
    "# - Dataset: derived/unified_dualtask (train/val/test CSVs)\n",
    "# - Spacing standardization to (0.8, 0.8, 1.0) mm\n",
    "# - Label-preserving resample via one-hot + optional dilate-then-erode\n",
    "# - Sliding-window patch training (192x192x160)\n",
    "# - Shared-encoder segmentation (DynUNet) + classification head\n",
    "# - QC counters that flag label shrinkage post-resample\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, decollate_batch\n",
    "from monai.data.utils import no_collation\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandAffined,\n",
    "    AsDiscreted,\n",
    "    EnsureTyped,\n",
    "    CastToTyped,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272457d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fee19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_determinism(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Paths\n",
    "PROJ_ROOT = Path('/home/qarc/projects/DHAI-Brain-Segmentation')\n",
    "DUALTASK_ROOT = PROJ_ROOT / 'derived' / 'unified_dualtask'\n",
    "TRAIN_CSV = DUALTASK_ROOT / 'train_fixed.csv'\n",
    "VAL_CSV = DUALTASK_ROOT / 'val_fixed.csv'\n",
    "TEST_CSV = DUALTASK_ROOT / 'test_fixed.csv'\n",
    "\n",
    "assert TRAIN_CSV.exists() and VAL_CSV.exists() and TEST_CSV.exists(), 'Split CSVs missing'\n",
    "\n",
    "# Target spacing and patch params\n",
    "TARGET_SPACING = (0.8, 0.8, 1.0)\n",
    "PATCH_SIZE = (192, 192, 160)\n",
    "PATCH_OVERLAP = 0.5  # sliding window overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV -> dict list helpers\n",
    "\n",
    "def read_unified_csv(path: Path) -> List[Dict]:\n",
    "    df = pd.read_csv(path)\n",
    "    # expected columns: case_id,class_label,image_path,label_path\n",
    "    items = []\n",
    "    for _, row in df.iterrows():\n",
    "        items.append({\n",
    "            'case_id': row['case_id'],\n",
    "            'image': row['image_path'],\n",
    "            'label': row['label_path'],\n",
    "            'class_label': int(row['class_label']),\n",
    "        })\n",
    "    return items\n",
    "\n",
    "train_items = read_unified_csv(TRAIN_CSV)\n",
    "val_items = read_unified_csv(VAL_CSV)\n",
    "test_items = read_unified_csv(TEST_CSV)\n",
    "\n",
    "len(train_items), len(val_items), len(test_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morphology utilities and QC counters\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "class LabelQC:\n",
    "    def __init__(self, shrink_warn_threshold: float = 0.35):\n",
    "        self.shrink_warn_threshold = shrink_warn_threshold\n",
    "        self.total = 0\n",
    "        self.warn = 0\n",
    "    def update(self, before_voxels: int, after_voxels: int, case_id: str):\n",
    "        self.total += 1\n",
    "        if before_voxels > 0:\n",
    "            ratio = (after_voxels + 1e-6) / (before_voxels + 1e-6)\n",
    "            if ratio < (1.0 - self.shrink_warn_threshold):\n",
    "                self.warn += 1\n",
    "                print(f'[QC] label shrinkage: {case_id} before={before_voxels} after={after_voxels} ratio={ratio:.3f}')\n",
    "    def summary(self):\n",
    "        print(f'[QC] shrinkage warnings: {self.warn}/{self.total}')\n",
    "\n",
    "\n",
    "def binary_dilate_then_erode(mask: np.ndarray, radius_vox: int = 1) -> np.ndarray:\n",
    "    if radius_vox <= 0:\n",
    "        return mask\n",
    "    structure = ndi.generate_binary_structure(3, 1)\n",
    "    for _ in range(radius_vox):\n",
    "        mask = ndi.binary_dilation(mask, structure=structure)\n",
    "    for _ in range(radius_vox):\n",
    "        mask = ndi.binary_erosion(mask, structure=structure)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms: spacing standardization and intensity scale\n",
    "# Labels are handled with a custom post-transform step to preserve small lesions via one-hot resample and optional morph.\n",
    "\n",
    "from monai.transforms import MapTransform\n",
    "\n",
    "class OneHotResampleWithMorphology(MapTransform):\n",
    "    def __init__(self, keys, num_classes: int = 2, morph_radius: int = 0, allow_missing_keys: bool = False):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.num_classes = num_classes\n",
    "        self.morph_radius = morph_radius\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        # expects d['label'] to be a MONAI image tensor with metadata spacing attached\n",
    "        label = d['label']  # torch.Tensor [1, D, H, W] after EnsureChannelFirstd\n",
    "        meta = d.get('label_meta_dict', {})\n",
    "        # Before voxels for QC\n",
    "        before_vox = int((label > 0.5).sum().item())\n",
    "\n",
    "        # one-hot\n",
    "        label_oh = F.one_hot(label.long().squeeze(0), num_classes=self.num_classes).permute(3, 0, 1, 2).float()\n",
    "\n",
    "        # resample using metadata of image (already resampled) to match size via trilinear/nearest\n",
    "        # assume image and label now share the same shape; if not, interpolate to image shape\n",
    "        img = d['image']\n",
    "        if label_oh.shape[1:] != img.shape[1:]:\n",
    "            # channels first\n",
    "            label_oh = F.interpolate(label_oh.unsqueeze(0), size=img.shape[1:], mode='trilinear', align_corners=False).squeeze(0)\n",
    "        # discretize back to argmax\n",
    "        label_res = label_oh.argmax(dim=0, keepdim=True)\n",
    "\n",
    "        # optional light morph\n",
    "        if self.morph_radius > 0:\n",
    "            arr = label_res.detach().cpu().numpy().astype(np.uint8)\n",
    "            arr = binary_dilate_then_erode(arr[0], radius_vox=self.morph_radius)[None]\n",
    "            label_res = torch.as_tensor(arr, dtype=torch.long, device=label_res.device)\n",
    "\n",
    "        after_vox = int((label_res > 0).sum().item())\n",
    "        d['label'] = label_res\n",
    "        d['qc_before_vox'] = before_vox\n",
    "        d['qc_after_vox'] = after_vox\n",
    "        return d\n",
    "\n",
    "TARGET_MODE = 'bilinear'\n",
    "\n",
    "common_load = [\n",
    "    LoadImaged(keys=['image', 'label']),\n",
    "    EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    EnsureTyped(keys=['image', 'label'], dtype=torch.float32),\n",
    "    Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "    Spacingd(keys=['image', 'label'], pixdim=TARGET_SPACING, mode=('bilinear', 'nearest')),\n",
    "]\n",
    "\n",
    "from monai.transforms import RandCropByPosNegLabeld, SpatialPadd\n",
    "\n",
    "intensity_train = [\n",
    "    ScaleIntensityRanged(keys=['image'], a_min=0, a_max=3000, b_min=0.0, b_max=1.0, clip=True),\n",
    "    RandFlipd(keys=['image', 'label'], spatial_axis=[0, 1, 2], prob=0.2),\n",
    "    RandRotate90d(keys=['image', 'label'], prob=0.2, max_k=3),\n",
    "    RandAffined(keys=['image', 'label'], rotate_range=(math.pi/36, math.pi/36, math.pi/36),\n",
    "                scale_range=(0.1, 0.1, 0.1), mode=('bilinear', 'nearest'), prob=0.2),\n",
    "    SpatialPadd(keys=['image', 'label'], spatial_size=PATCH_SIZE),\n",
    "    RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', spatial_size=PATCH_SIZE,\n",
    "                           pos=1, neg=1, num_samples=1, image_key='image', allow_smaller=True),\n",
    "]\n",
    "\n",
    "intensity_val = [\n",
    "    ScaleIntensityRanged(keys=['image'], a_min=0, a_max=3000, b_min=0.0, b_max=1.0, clip=True),\n",
    "]\n",
    "\n",
    "# Post label discretization with morphology + QC bookkeeping performed inline by OneHotResampleWithMorphology\n",
    "\n",
    "post_label_preserve = [\n",
    "    OneHotResampleWithMorphology(keys=['label'], num_classes=2, morph_radius=1),\n",
    "]\n",
    "\n",
    "class CastClassLabeld(MapTransform):\n",
    "    def __init__(self, keys, allow_missing_keys=False):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        if 'class_label' in d:\n",
    "            d['class_label'] = torch.as_tensor(d['class_label'], dtype=torch.float32)\n",
    "        return d\n",
    "\n",
    "train_transforms = Compose(common_load + intensity_train + post_label_preserve + [CastClassLabeld(keys=['class_label'])])\n",
    "val_transforms = Compose(common_load + intensity_val + post_label_preserve + [CastClassLabeld(keys=['class_label'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97956c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Loaders with QC hooks\n",
    "\n",
    "qc_train = LabelQC(shrink_warn_threshold=0.35)\n",
    "qc_val = LabelQC(shrink_warn_threshold=0.35)\n",
    "\n",
    "class QCAugmentWrapper(CacheDataset):\n",
    "    def __init__(self, data, transform, cache_rate=0.0, num_workers=4, copy_cache=True):\n",
    "        super().__init__(data=data, transform=transform, cache_rate=cache_rate, num_workers=num_workers, copy_cache=copy_cache)\n",
    "    def __getitem__(self, index):\n",
    "        item = super().__getitem__(index)\n",
    "        # Collect QC counters if present\n",
    "        case_id = item.get('case_id') if isinstance(item, dict) else None\n",
    "        before_vox = int(item.get('qc_before_vox', 0))\n",
    "        after_vox = int(item.get('qc_after_vox', 0))\n",
    "        if before_vox or after_vox:\n",
    "            # decide which QC to update based on internal flag set earlier\n",
    "            pass\n",
    "        return item\n",
    "\n",
    "train_ds = CacheDataset(data=train_items, transform=train_transforms, cache_rate=0.0, num_workers=4)\n",
    "val_ds = CacheDataset(data=val_items, transform=val_transforms, cache_rate=0.0, num_workers=2)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=1, pin_memory=False, persistent_workers=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=False, persistent_workers=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddff3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: DynUNet backbone + classification head from encoder bottleneck\n",
    "\n",
    "# Segmentation network\n",
    "seg_net = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    kernel_size=[3, 3, 3, 3, 3, 3],\n",
    "    strides=[1, 2, 2, 2, 2, 2],\n",
    "    upsample_kernel_size=[2, 2, 2, 2, 2],\n",
    "    norm_name='instance',\n",
    "    deep_supervision=False,\n",
    ").to(device)\n",
    "\n",
    "# Classification head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "    def forward(self, feat):\n",
    "        x = self.pool(feat).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Identify bottleneck channels from DynUNet\n",
    "# DynUNet returns a list of decoder outputs when deep_supervision; we also can hook encoder features via register_forward_hook\n",
    "\n",
    "# Lazy-init classification head once we know feature channels\n",
    "# Update the LazyClassificationHead class\n",
    "class LazyClassificationHead(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = None\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, feat):\n",
    "        x = self.pool(feat).flatten(1)\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(x.shape[1], self.num_classes).to(x.device)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        \"\"\"Override load_state_dict to handle lazy initialization\"\"\"\n",
    "        # Check if we have fc weights in the state dict\n",
    "        if 'fc.weight' in state_dict and 'fc.bias' in state_dict:\n",
    "            # Initialize fc layer with the saved dimensions\n",
    "            fc_weight = state_dict['fc.weight']\n",
    "            fc_bias = state_dict['fc.bias']\n",
    "            \n",
    "            # Create fc layer with correct dimensions\n",
    "            if self.fc is None:\n",
    "                in_features = fc_weight.shape[1]\n",
    "                self.fc = nn.Linear(in_features, self.num_classes).to(fc_weight.device)\n",
    "            \n",
    "            # Load the weights\n",
    "            self.fc.weight.data = fc_weight\n",
    "            self.fc.bias.data = fc_bias\n",
    "            \n",
    "            # Remove fc keys from state_dict to avoid double loading\n",
    "            state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.')}\n",
    "            \n",
    "        # Load any remaining state dict items\n",
    "        if state_dict:\n",
    "            super().load_state_dict(state_dict, strict=strict)\n",
    "        \n",
    "        return None  # Return None for compatibility with older PyTorch versions\n",
    "\n",
    "cls_head = LazyClassificationHead(num_classes=1).to(device)\n",
    "\n",
    "# Simple hook to capture bottleneck features\n",
    "encoder_feat = {'x': None}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    encoder_feat['x'] = output\n",
    "\n",
    "# Attach hook to bottleneck layer (seg_net.encoder4 or seg_net.bottleneck depending on version)\n",
    "if hasattr(seg_net, 'bottleneck'):\n",
    "    seg_net.bottleneck.register_forward_hook(hook_fn)\n",
    "elif hasattr(seg_net, 'encoder4'):\n",
    "    seg_net.encoder4.register_forward_hook(hook_fn)\n",
    "else:\n",
    "    print('[WARN] Could not attach hook, classification head may not receive features')\n",
    "\n",
    "# Losses and optimizer\n",
    "seg_loss_fn = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "cls_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "params = list(seg_net.parameters()) + list(cls_head.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=2e-4, weight_decay=1e-5)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Metrics\n",
    "post_pred = AsDiscreted(keys=['pred'], argmax=True)\n",
    "post_label = AsDiscreted(keys=['label'], to_onehot=2)\n",
    "dice_metric = DiceMetric(include_background=False, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: pad tensor to next multiple-of factor for each spatial dim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_factor(x: torch.Tensor, factor: int = 32) -> torch.Tensor:\n",
    "    # x: (B, C, D, H, W)\n",
    "    B, C, D, H, W = x.shape\n",
    "    def next_m(s):\n",
    "        return ((s + factor - 1) // factor) * factor\n",
    "    Dn, Hn, Wn = next_m(D), next_m(H), next_m(W)\n",
    "    pd = Dn - D; ph = Hn - H; pw = Wn - W\n",
    "    # pad order: (W_left, W_right, H_left, H_right, D_left, D_right)\n",
    "    pad = (0, pw, 0, ph, 0, pd)\n",
    "    if any(p > 0 for p in pad):\n",
    "        x = F.pad(x, pad, mode='constant', value=0.0)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62935e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferer for validation/test\n",
    "inferer = SlidingWindowInferer(roi_size=PATCH_SIZE, sw_batch_size=1, overlap=PATCH_OVERLAP, mode='gaussian')\n",
    "\n",
    "# Utils\n",
    "from contextlib import nullcontext\n",
    "\n",
    "def to_device(batch: Dict, device: torch.device) -> Dict:\n",
    "    \"\"\"Enhanced to_device with error handling\"\"\"\n",
    "    out = {}\n",
    "    try:\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                # Check if tensor is valid\n",
    "                if torch.isnan(v).any() or torch.isinf(v).any():\n",
    "                    print(f\"Warning: Invalid tensor detected in {k}\")\n",
    "                    continue\n",
    "                \n",
    "                # Move to device with error handling\n",
    "                try:\n",
    "                    out[k] = v.to(device, non_blocking=True)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error moving {k} to device: {e}\")\n",
    "                    # Try synchronous transfer as fallback\n",
    "                    try:\n",
    "                        out[k] = v.to(device, non_blocking=False)\n",
    "                    except RuntimeError as e2:\n",
    "                        print(f\"Fallback transfer also failed for {k}: {e2}\")\n",
    "                        # Keep original tensor on CPU as last resort\n",
    "                        out[k] = v\n",
    "                        continue\n",
    "            else:\n",
    "                out[k] = v\n",
    "    except Exception as e:\n",
    "        print(f\"Error in to_device: {e}\")\n",
    "        # Return original batch if device transfer fails\n",
    "        return batch\n",
    "    \n",
    "    return out\n",
    "\n",
    "ckpt_dir = PROJ_ROOT / 'runs' / 'dualtask_monai'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "print('Checkpoint dir:', ckpt_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced metrics imports and utilities\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Enhanced metrics calculation functions\n",
    "def calculate_classification_metrics(y_true: List[int], y_prob: List[float], y_pred: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate classification metrics: accuracy, F1, AUC\"\"\"\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "def calculate_segmentation_metrics(pred: torch.Tensor, target: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Calculate segmentation metrics: Dice and Hausdorff distance\"\"\"\n",
    "    try:\n",
    "        # Convert to numpy for Hausdorff calculation\n",
    "        pred_np = pred.detach().cpu().numpy()\n",
    "        target_np = target.detach().cpu().numpy()\n",
    "        \n",
    "        # Ensure binary masks\n",
    "        pred_binary = (pred_np > 0.5).astype(np.uint8)\n",
    "        target_binary = (target_np > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Calculate Hausdorff distance only if both masks have foreground\n",
    "        if pred_binary.sum() > 0 and target_binary.sum() > 0:\n",
    "            # Use MONAI's HausdorffDistanceMetric\n",
    "            hausdorff_metric = HausdorffDistanceMetric(include_background=False, percentile=95.0)\n",
    "            hausdorff_metric(y_pred=torch.from_numpy(pred_binary), y=torch.from_numpy(target_binary))\n",
    "            hausdorff_dist = hausdorff_metric.aggregate().item()\n",
    "        else:\n",
    "            hausdorff_dist = float('nan')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate Hausdorff distance: {e}\")\n",
    "        hausdorff_dist = float('nan')\n",
    "    \n",
    "    return {\n",
    "        'hausdorff_distance': hausdorff_dist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive checkpointing and resume functionality\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "# Function to initialize classification head with dummy data\n",
    "def initialize_classification_head(cls_head, seg_net, device):\n",
    "    \"\"\"Initialize classification head with dummy forward pass\"\"\"\n",
    "    print(\"Initializing classification head...\")\n",
    "    \n",
    "    # Create dummy input to trigger lazy initialization\n",
    "    dummy_input = torch.randn(1, 1, 64, 64, 64).to(device)  # Small dummy volume\n",
    "    \n",
    "    # Forward pass through segmentation network to get features\n",
    "    with torch.no_grad():\n",
    "        _ = seg_net(dummy_input)\n",
    "        # Get bottleneck features\n",
    "        if 'encoder_feat' in globals() and encoder_feat['x'] is not None:\n",
    "            dummy_feat = encoder_feat['x']\n",
    "        else:\n",
    "            # Fallback: use a dummy feature tensor\n",
    "            dummy_feat = torch.randn(1, 256, 8, 8, 8).to(device)  # Typical bottleneck size\n",
    "        \n",
    "        # This will initialize the fc layer\n",
    "        _ = cls_head(dummy_feat)\n",
    "    \n",
    "    print(\"Classification head initialized\")\n",
    "class TrainingStateManager:\n",
    "    def __init__(self, checkpoint_dir: Path):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_dice': [],\n",
    "            'val_hausdorff': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1': [],\n",
    "            'val_auc': []\n",
    "        }\n",
    "        \n",
    "    def save_checkpoint(self, epoch: int, seg_net, cls_head, optimizer, scaler, \n",
    "                       best_val_dice: float, current_metrics: Dict, is_best: bool = False):\n",
    "        \"\"\"Save comprehensive training checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'seg_net_state_dict': seg_net.state_dict(),\n",
    "            'cls_head_state_dict': cls_head.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_val_dice': best_val_dice,\n",
    "            'current_metrics': current_metrics,\n",
    "            'history': self.history,\n",
    "            'random_state': torch.get_rng_state(),\n",
    "            'numpy_random_state': np.random.get_state(),\n",
    "            'python_random_state': random.getstate(),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # Save best checkpoint\n",
    "        if is_best:\n",
    "            best_path = self.checkpoint_dir / 'best_checkpoint.pt'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            \n",
    "        # Save latest checkpoint (for easy resuming)\n",
    "        latest_path = self.checkpoint_dir / 'latest_checkpoint.pt'\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        if is_best:\n",
    "            print(f\"Best checkpoint updated: {best_path}\")\n",
    "            \n",
    "    # Update the load_checkpoint method in TrainingStateManager\n",
    "    def load_checkpoint(self, seg_net, cls_head, optimizer, scaler, checkpoint_path: str = None):\n",
    "        \"\"\"Load checkpoint and restore training state\"\"\"\n",
    "        if checkpoint_path is None:\n",
    "            checkpoint_path = self.checkpoint_dir / 'latest_checkpoint.pt'\n",
    "            \n",
    "        if not Path(checkpoint_path).exists():\n",
    "            print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "            return 0, -1.0\n",
    "            \n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        \n",
    "        try:\n",
    "            # First try with weights_only=True (safe loading)\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "            print(\"Loaded checkpoint with safe loading\")\n",
    "        except Exception as e:\n",
    "            print(f\"Safe loading failed: {e}\")\n",
    "            print(\"Attempting to load with full state restoration...\")\n",
    "            \n",
    "            # If safe loading fails, try with weights_only=False (trusted source)\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "            print(\"Loaded checkpoint with full state restoration\")\n",
    "        \n",
    "        # Initialize classification head if needed\n",
    "        if cls_head.fc is None:\n",
    "            print(\"Initializing classification head before loading weights...\")\n",
    "            initialize_classification_head(cls_head, seg_net, device)\n",
    "        \n",
    "        # Restore model states\n",
    "        try:\n",
    "            seg_net.load_state_dict(checkpoint['seg_net_state_dict'])\n",
    "            print(\"Segmentation network loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading segmentation network: {e}\")\n",
    "            raise\n",
    "            \n",
    "        try:\n",
    "            cls_head.load_state_dict(checkpoint['cls_head_state_dict'])\n",
    "            print(\"Classification head loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading classification head: {e}\")\n",
    "            raise\n",
    "            \n",
    "        try:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"Optimizer state loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading optimizer state: {e}\")\n",
    "            raise\n",
    "            \n",
    "        try:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            print(\"GradScaler state loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading GradScaler state: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Improved random state restoration\n",
    "        try:\n",
    "            if 'random_state' in checkpoint:\n",
    "                random_state = checkpoint['random_state']\n",
    "                # Ensure it's on the correct device and has correct type\n",
    "                if isinstance(random_state, torch.Tensor):\n",
    "                    if random_state.device != device:\n",
    "                        random_state = random_state.to(device)\n",
    "                    if random_state.dtype != torch.uint8:\n",
    "                        random_state = random_state.to(torch.uint8)\n",
    "                    torch.set_rng_state(random_state)\n",
    "                    print(\"PyTorch random state restored\")\n",
    "                else:\n",
    "                    print(\"Warning: Invalid PyTorch random state format\")\n",
    "                    \n",
    "            if 'numpy_random_state' in checkpoint:\n",
    "                np.random.set_state(checkpoint['numpy_random_state'])\n",
    "                print(\"NumPy random state restored\")\n",
    "                \n",
    "            if 'python_random_state' in checkpoint:\n",
    "                random.setstate(checkpoint['python_random_state'])\n",
    "                print(\"Python random state restored\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not restore random states: {e}\")\n",
    "            print(\"Setting new random seed for continued training...\")\n",
    "            # Set a new deterministic seed\n",
    "            new_seed = 42 + checkpoint.get('epoch', 0)  # Different seed for each epoch\n",
    "            set_determinism(new_seed)\n",
    "            print(f\"New random seed set: {new_seed}\")\n",
    "        \n",
    "        # Restore training history\n",
    "        if 'history' in checkpoint:\n",
    "            self.history = checkpoint['history']\n",
    "            print(\"Training history restored\")\n",
    "        else:\n",
    "            print(\"No training history found in checkpoint\")\n",
    "        \n",
    "        epoch = checkpoint['epoch']\n",
    "        best_val_dice = checkpoint['best_val_dice']\n",
    "        \n",
    "        print(f\"Resumed from epoch {epoch}, best val dice: {best_val_dice:.4f}\")\n",
    "        return epoch, best_val_dice\n",
    "    \n",
    "    def update_history(self, train_loss: float, val_loss: float = None, \n",
    "                      val_dice: float = None, val_hausdorff: float = None,\n",
    "                      val_accuracy: float = None, val_f1: float = None, val_auc: float = None):\n",
    "        \"\"\"Update training history\"\"\"\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        if val_loss is not None:\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "        if val_dice is not None:\n",
    "            self.history['val_dice'].append(val_dice)\n",
    "        if val_hausdorff is not None:\n",
    "            self.history['val_hausdorff'].append(val_hausdorff)\n",
    "        if val_accuracy is not None:\n",
    "            self.history['val_accuracy'].append(val_accuracy)\n",
    "        if val_f1 is not None:\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "        if val_auc is not None:\n",
    "            self.history['val_auc'].append(val_auc)\n",
    "\n",
    "# Initialize training state manager\n",
    "state_manager = TrainingStateManager(ckpt_dir)\n",
    "\n",
    "# Check for existing checkpoint to resume from\n",
    "start_epoch, best_val_dice = state_manager.load_checkpoint(seg_net, cls_head, optimizer, scaler)\n",
    "if start_epoch == 0:\n",
    "    best_val_dice = -1.0\n",
    "    print(\"Starting training from scratch\")\n",
    "else:\n",
    "    print(f\"Resuming training from epoch {start_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32260d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Enhanced Train / Val loops with comprehensive metrics, checkpointing, and memory management\n",
    "EPOCHS = 50\n",
    "val_interval = 1\n",
    "save_checkpoint_interval = 5  # Save checkpoint every 5 epochs\n",
    "\n",
    "# Initialize additional metrics\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=False, percentile=95.0)\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\")\n",
    "\n",
    "def cleanup_batch_memory(images=None, labels=None, seg_logits=None, cls_logits=None, feat=None, images_p=None):\n",
    "    \"\"\"Clean up batch-related tensors to free memory\"\"\"\n",
    "    del_list = [images, labels, seg_logits, cls_logits, feat, images_p]\n",
    "    for tensor in del_list:\n",
    "        if tensor is not None:\n",
    "            del tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "for epoch in range(start_epoch + 1, EPOCHS + 1):\n",
    "    seg_net.train(); cls_head.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_steps = 0\n",
    "    \n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    monitor_gpu_memory()\n",
    "\n",
    "    # Training loop with memory management\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Memory cleanup every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # QC update per-sample\n",
    "        for b in decollate_batch(batch):\n",
    "            qc_train.update(int(b.get('qc_before_vox', 0)), int(b.get('qc_after_vox', 0)), str(b.get('case_id', '?')))\n",
    "        \n",
    "        batch = to_device(batch, device)\n",
    "        images = batch['image']\n",
    "        labels = batch['label'].long()\n",
    "        class_labels = batch['class_label'].view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        ctx = autocast(device_type='cuda', enabled=torch.cuda.is_available())\n",
    "        with ctx:\n",
    "            encoder_feat['x'] = None\n",
    "            seg_logits = seg_net(images)\n",
    "            # DynUNet with deep supervision returns list; last is highest res\n",
    "            if isinstance(seg_logits, (list, tuple)):\n",
    "                seg_logits_main = seg_logits\n",
    "            else:\n",
    "                seg_logits_main = seg_logits\n",
    "            # Classification\n",
    "            feat = encoder_feat['x'] if encoder_feat['x'] is not None else seg_logits_main\n",
    "            cls_logits = cls_head(feat)\n",
    "\n",
    "            loss_seg = seg_loss_fn(seg_logits_main, labels)\n",
    "            loss_cls = cls_loss_fn(cls_logits, class_labels)\n",
    "            loss = loss_seg + 0.3 * loss_cls\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_steps += 1\n",
    "        \n",
    "        # Cleanup training batch memory\n",
    "        cleanup_batch_memory(images, labels, seg_logits_main, cls_logits, feat)\n",
    "\n",
    "    epoch_loss /= max(1, num_steps)\n",
    "    print(f'Epoch {epoch}/{EPOCHS} - train loss: {epoch_loss:.4f}')\n",
    "    if epoch % val_interval == 0:\n",
    "        qc_train.summary()\n",
    "\n",
    "    # Validation loop with memory management\n",
    "    if epoch % val_interval == 0:\n",
    "        seg_net.eval(); cls_head.eval()\n",
    "        dice_metric.reset()\n",
    "        hausdorff_metric.reset()\n",
    "        val_loss = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        # For classification metrics\n",
    "        val_y_true, val_y_prob = [], []\n",
    "        val_hausdorff_distances = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx, batch in enumerate(val_loader):\n",
    "                # Memory cleanup every 5 validation batches\n",
    "                if val_batch_idx % 5 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                for b in decollate_batch(batch):\n",
    "                    qc_val.update(int(b.get('qc_before_vox', 0)), int(b.get('qc_after_vox', 0)), str(b.get('case_id', '?')))\n",
    "                \n",
    "                batch = to_device(batch, device)\n",
    "                images = batch['image']\n",
    "                labels = batch['label'].long()\n",
    "                class_labels = batch['class_label'].view(-1, 1)\n",
    "\n",
    "                ctx = autocast(device_type='cuda', enabled=torch.cuda.is_available())\n",
    "                with ctx:\n",
    "                    images_p = pad_to_factor(images, factor=32)  # (B,C,D,H,W) -> padded to mult of 32\n",
    "                    seg_logits = inferer(inputs=images_p, network=seg_net)\n",
    "                    encoder_feat['x'] = None    \n",
    "                    # classification from encoder feature may not be available in inferer path; do a forward to populate\n",
    "                    _ = seg_net(images_p)\n",
    "                    if seg_logits.shape[-3:] != labels.shape[-3:]:\n",
    "                        Dz, Hy, Wx = labels.shape[-3:]\n",
    "                        seg_logits = seg_logits[..., :Dz, :Hy, :Wx]\n",
    "                    feat = encoder_feat['x'] if encoder_feat['x'] is not None else seg_logits\n",
    "                    cls_logits = cls_head(feat)\n",
    "\n",
    "                    loss_seg = seg_loss_fn(seg_logits, labels)\n",
    "                    loss_cls = cls_loss_fn(cls_logits, class_labels)\n",
    "                    loss = loss_seg + 0.3 * loss_cls\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                steps += 1\n",
    "\n",
    "                # Segmentation metrics\n",
    "                y_pred = torch.softmax(seg_logits, dim=1)\n",
    "                y_pred_discrete = torch.argmax(y_pred, dim=1, keepdim=True)\n",
    "                dice_metric(y_pred=y_pred_discrete, y=labels)\n",
    "                \n",
    "                # Hausdorff distance calculation\n",
    "                try:\n",
    "                    hausdorff_metric(y_pred=y_pred_discrete, y=labels)\n",
    "                    hausdorff_dist = hausdorff_metric.aggregate().item()\n",
    "                    val_hausdorff_distances.append(hausdorff_dist)\n",
    "                    hausdorff_metric.reset()  # Reset for next sample\n",
    "                except:\n",
    "                    val_hausdorff_distances.append(float('nan'))\n",
    "\n",
    "                # Classification metrics collection\n",
    "                val_y_true.append(int(class_labels.item()))\n",
    "                val_y_prob.append(torch.sigmoid(cls_logits).item())\n",
    "                \n",
    "                # Cleanup validation batch memory\n",
    "                cleanup_batch_memory(images, labels, seg_logits, cls_logits, feat, images_p)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mean_dice = dice_metric.aggregate().item()\n",
    "        val_loss /= max(1, steps)\n",
    "        mean_hausdorff = np.nanmean(val_hausdorff_distances) if val_hausdorff_distances else float('nan')\n",
    "        \n",
    "        # Classification metrics\n",
    "        val_y_pred = [1 if p >= 0.5 else 0 for p in val_y_prob]\n",
    "        cls_metrics = calculate_classification_metrics(val_y_true, val_y_prob, val_y_pred)\n",
    "        \n",
    "        # Print comprehensive metrics\n",
    "        print(f'  Val loss: {val_loss:.4f} | Val Dice: {mean_dice:.4f} | Val Hausdorff: {mean_hausdorff:.2f}')\n",
    "        print(f'  Val Accuracy: {cls_metrics[\"accuracy\"]:.4f} | Val F1: {cls_metrics[\"f1_score\"]:.4f} | Val AUC: {cls_metrics[\"auc\"]:.4f}')\n",
    "        qc_val.summary()\n",
    "        \n",
    "        # Update history\n",
    "        state_manager.update_history(\n",
    "            train_loss=epoch_loss,\n",
    "            val_loss=val_loss,\n",
    "            val_dice=mean_dice,\n",
    "            val_hausdorff=mean_hausdorff,\n",
    "            val_accuracy=cls_metrics[\"accuracy\"],\n",
    "            val_f1=cls_metrics[\"f1_score\"],\n",
    "            val_auc=cls_metrics[\"auc\"]\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint\n",
    "        current_metrics = {\n",
    "            'val_loss': val_loss,\n",
    "            'val_dice': mean_dice,\n",
    "            'val_hausdorff': mean_hausdorff,\n",
    "            'val_accuracy': cls_metrics[\"accuracy\"],\n",
    "            'val_f1': cls_metrics[\"f1_score\"],\n",
    "            'val_auc': cls_metrics[\"auc\"]\n",
    "        }\n",
    "        \n",
    "        is_best = mean_dice > best_val_dice\n",
    "        if is_best:\n",
    "            best_val_dice = mean_dice\n",
    "            print(f'  [NEW BEST] Dice: {best_val_dice:.4f}')\n",
    "            \n",
    "        # Save checkpoint (best + regular interval)\n",
    "        if is_best or epoch % save_checkpoint_interval == 0:\n",
    "            state_manager.save_checkpoint(\n",
    "                epoch=epoch,\n",
    "                seg_net=seg_net,\n",
    "                cls_head=cls_head,\n",
    "                optimizer=optimizer,\n",
    "                scaler=scaler,\n",
    "                best_val_dice=best_val_dice,\n",
    "                current_metrics=current_metrics,\n",
    "                is_best=is_best\n",
    "            )\n",
    "\n",
    "        # Final cleanup after validation\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        monitor_gpu_memory()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03632ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced test evaluation with comprehensive metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Build test loader\n",
    "test_ds = CacheDataset(data=test_items, transform=val_transforms, cache_rate=0.0, num_workers=2)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Load best checkpoint\n",
    "best_ckpt_path = ckpt_dir / 'best_checkpoint.pt'\n",
    "if best_ckpt_path.exists():\n",
    "    checkpoint = torch.load(best_ckpt_path, map_location=device)\n",
    "    seg_net.load_state_dict(checkpoint['seg_net_state_dict'])\n",
    "    cls_head.load_state_dict(checkpoint['cls_head_state_dict'])\n",
    "    print(f\"Loaded best checkpoint from epoch {checkpoint['epoch']}\")\n",
    "else:\n",
    "    print(\"No best checkpoint found, using current model weights\")\n",
    "\n",
    "seg_net.eval(); cls_head.eval()\n",
    "\n",
    "dice_metric.reset()\n",
    "hausdorff_metric.reset()\n",
    "test_y_true, test_y_prob = [], []\n",
    "test_hausdorff_distances = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        images = batch['image']\n",
    "        labels = batch['label'].long()\n",
    "        class_labels = batch['class_label'].view(-1, 1)\n",
    "\n",
    "        ctx = autocast(device_type='cuda', enabled=torch.cuda.is_available())\n",
    "        with ctx:\n",
    "            seg_logits = inferer(inputs=images, network=seg_net)\n",
    "            _ = seg_net(images)\n",
    "            feat = encoder_feat['x'] if encoder_feat['x'] is not None else seg_logits\n",
    "            cls_logits = cls_head(feat)\n",
    "\n",
    "            # Segmentation metrics\n",
    "            y_pred = torch.softmax(seg_logits, dim=1)\n",
    "            y_pred_discrete = torch.argmax(y_pred, dim=1, keepdim=True)\n",
    "            dice_metric(y_pred=y_pred_discrete, y=labels)\n",
    "            \n",
    "            # Hausdorff distance\n",
    "            try:\n",
    "                hausdorff_metric(y_pred=y_pred_discrete, y=labels)\n",
    "                hausdorff_dist = hausdorff_metric.aggregate().item()\n",
    "                test_hausdorff_distances.append(hausdorff_dist)\n",
    "                hausdorff_metric.reset()\n",
    "            except:\n",
    "                test_hausdorff_distances.append(float('nan'))\n",
    "\n",
    "            # Classification metrics collection\n",
    "            test_y_true.append(int(class_labels.item()))\n",
    "            test_y_prob.append(torch.sigmoid(cls_logits).item())\n",
    "\n",
    "# Calculate final test metrics\n",
    "test_dice = dice_metric.aggregate().item()\n",
    "test_hausdorff = np.nanmean(test_hausdorff_distances) if test_hausdorff_distances else float('nan')\n",
    "\n",
    "test_y_pred = [1 if p >= 0.5 else 0 for p in test_y_prob]\n",
    "test_cls_metrics = calculate_classification_metrics(test_y_true, test_y_prob, test_y_pred)\n",
    "\n",
    "# Print comprehensive test results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Segmentation Metrics:\")\n",
    "print(f\"  - Dice Score: {test_dice:.4f}\")\n",
    "print(f\"  - Hausdorff Distance: {test_hausdorff:.2f}\")\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"  - Accuracy: {test_cls_metrics['accuracy']:.4f}\")\n",
    "print(f\"  - F1 Score: {test_cls_metrics['f1_score']:.4f}\")\n",
    "print(f\"  - AUC: {test_cls_metrics['auc']:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save final results\n",
    "final_results = {\n",
    "    'test_dice': test_dice,\n",
    "    'test_hausdorff': test_hausdorff,\n",
    "    'test_accuracy': test_cls_metrics['accuracy'],\n",
    "    'test_f1': test_cls_metrics['f1_score'],\n",
    "    'test_auc': test_cls_metrics['auc'],\n",
    "    'training_history': state_manager.history\n",
    "}\n",
    "\n",
    "results_path = ckpt_dir / 'final_test_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive plotting and analysis for publication\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def create_publication_plots(history: Dict, save_dir: Path):\n",
    "    \"\"\"Create publication-quality plots for all training metrics\"\"\"\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Training and Validation Loss\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training Loss', marker='o', markersize=4)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss', marker='s', markersize=4)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Segmentation Metrics\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.plot(epochs, history['val_dice'], 'g-', linewidth=2, label='Dice Score', marker='^', markersize=4)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Dice Score', fontsize=12)\n",
    "    ax2.set_title('Validation Dice Score', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Classification Metrics\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.plot(epochs, history['val_accuracy'], 'purple', linewidth=2, label='Accuracy', marker='o', markersize=4)\n",
    "    ax3.plot(epochs, history['val_f1'], 'orange', linewidth=2, label='F1 Score', marker='s', markersize=4)\n",
    "    ax3.plot(epochs, history['val_auc'], 'brown', linewidth=2, label='AUC', marker='^', markersize=4)\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Score', fontsize=12)\n",
    "    ax3.set_title('Classification Metrics', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Hausdorff Distance\n",
    "    ax4 = fig.add_subplot(gs[2, :2])\n",
    "    ax4.plot(epochs, history['val_hausdorff'], 'red', linewidth=2, label='Hausdorff Distance', marker='o', markersize=4)\n",
    "    ax4.set_xlabel('Epoch', fontsize=12)\n",
    "    ax4.set_ylabel('Distance (mm)', fontsize=12)\n",
    "    ax4.set_title('Validation Hausdorff Distance', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Combined Performance Overview\n",
    "    ax5 = fig.add_subplot(gs[2, 2])\n",
    "    # Normalize metrics to 0-1 range for comparison\n",
    "    dice_norm = np.array(history['val_dice'])\n",
    "    acc_norm = np.array(history['val_accuracy'])\n",
    "    f1_norm = np.array(history['val_f1'])\n",
    "    auc_norm = np.array(history['val_auc'])\n",
    "    \n",
    "    # Create radar chart data\n",
    "    categories = ['Dice', 'Accuracy', 'F1', 'AUC']\n",
    "    values = [dice_norm[-1], acc_norm[-1], f1_norm[-1], auc_norm[-1]]  # Final epoch values\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values += values[:1]  # Close the loop\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax5.plot(angles, values, 'o-', linewidth=2, color='blue')\n",
    "    ax5.fill(angles, values, alpha=0.25, color='blue')\n",
    "    ax5.set_xticks(angles[:-1])\n",
    "    ax5.set_xticklabels(categories)\n",
    "    ax5.set_ylim(0, 1)\n",
    "    ax5.set_title('Final Performance Overview', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution plots\n",
    "    plot_path = save_dir / 'training_metrics_publication.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Publication plots saved to: {plot_path}\")\n",
    "    \n",
    "    # Save individual plots\n",
    "    individual_plots = {\n",
    "        'loss_comparison': (gs[0, :2], 'loss_comparison.png'),\n",
    "        'dice_score': (gs[0, 2], 'dice_score.png'),\n",
    "        'classification_metrics': (gs[1, :], 'classification_metrics.png'),\n",
    "        'hausdorff_distance': (gs[2, :2], 'hausdorff_distance.png'),\n",
    "        'performance_overview': (gs[2, 2], 'performance_overview.png')\n",
    "    }\n",
    "    \n",
    "    for name, (gs_pos, filename) in individual_plots.items():\n",
    "        fig_ind = plt.figure(figsize=(8, 6))\n",
    "        ax_ind = fig_ind.add_subplot(111)\n",
    "        \n",
    "        # Copy the subplot content\n",
    "        ax_ind.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training Loss', marker='o', markersize=4)\n",
    "        ax_ind.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss', marker='s', markersize=4)\n",
    "        ax_ind.set_xlabel('Epoch', fontsize=12)\n",
    "        ax_ind.set_ylabel('Loss', fontsize=12)\n",
    "        ax_ind.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "        ax_ind.legend(fontsize=11)\n",
    "        ax_ind.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_path = save_dir / filename\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close(fig_ind)\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Generate publication plots\n",
    "if 'state_manager' in locals() and hasattr(state_manager, 'history'):\n",
    "    print(\"Generating publication-quality plots...\")\n",
    "    fig = create_publication_plots(state_manager.history, ckpt_dir)\n",
    "    \n",
    "    # Save plot data for external plotting tools (e.g., LaTeX, R)\n",
    "    plot_data = {\n",
    "        'epochs': list(range(1, len(state_manager.history['train_loss']) + 1)),\n",
    "        'metrics': state_manager.history\n",
    "    }\n",
    "    \n",
    "    plot_data_path = ckpt_dir / 'plot_data.json'\n",
    "    with open(plot_data_path, 'w') as f:\n",
    "        json.dump(plot_data, f, indent=2)\n",
    "    print(f\"Plot data saved to: {plot_data_path}\")\n",
    "    \n",
    "    # Generate summary statistics table\n",
    "    final_epoch = len(state_manager.history['train_loss'])\n",
    "    summary_stats = {\n",
    "        'final_epoch': final_epoch,\n",
    "        'best_val_dice': max(state_manager.history['val_dice']),\n",
    "        'best_val_dice_epoch': np.argmax(state_manager.history['val_dice']) + 1,\n",
    "        'final_val_accuracy': state_manager.history['val_accuracy'][-1],\n",
    "        'final_val_f1': state_manager.history['val_f1'][-1],\n",
    "        'final_val_auc': state_manager.history['val_auc'][-1],\n",
    "        'final_val_hausdorff': state_manager.history['val_hausdorff'][-1],\n",
    "        'training_convergence': {\n",
    "            'train_loss_start': state_manager.history['train_loss'][0],\n",
    "            'train_loss_end': state_manager.history['train_loss'][-1],\n",
    "            'val_loss_start': state_manager.history['val_loss'][0],\n",
    "            'val_loss_end': state_manager.history['val_loss'][-1]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = ckpt_dir / 'training_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    print(f\"Training summary saved to: {summary_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No training history found. Run training first to generate plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290a4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9bef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6005b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407c142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5dc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f0261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eca81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380fe72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18b536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7690ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f2974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e850f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b555e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925bd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
