{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af652a0-98a7-494b-9021-6deab2487371",
   "metadata": {},
   "source": [
    "# Project: Brain Tumor Segmentation and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a3a1c-f95f-4409-8d1d-bfb4a24e96c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Details of Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c40a1a-ba59-4cba-b244-9d416297fcbb",
   "metadata": {},
   "source": [
    "#### [Data Source - The Cancer Imaging Archive - TCIA](https://www.cancerimagingarchive.net/browse-collections)\n",
    "\n",
    "- **UCSF-PGDM:** Glioblastoma - 495\n",
    "- **BRATS-AFRICA:** Glioma - 95\n",
    "- **MU-Glioma-Post:** Glioma - 203\n",
    "- **UCSD-PTGBM:** Glioblastoma - 178\n",
    "- **UPENN-GBM:** Glioblastoma - 630\n",
    "- **BCBM-RadioGenomics:** Brain Mets - 165\n",
    "- **Pretreat-MetsToBrain-Masks:** Brain Mets - 200\n",
    "\n",
    "### Segmentation Dataset\n",
    "- **Numbers:** *495+95+203+178+630+165+200 = 1966*. But desired segmentation along with desired MRI sequence was present for - **1388** \n",
    "- **Segmentation:** Tumor core plus Enhancing area, Single segmentation mask and where two masks were provided like tumor core and enhancing area, then both the mask was combined and a single mask was derived - tumor with enhnacing area\n",
    "- **MRI sequence** used was - *T1 Contrast*\n",
    "- File *dataset.json* was created - Details of file path for MRI sequence and Segmentation and other dataset details\n",
    "\n",
    "### Classification dataset\n",
    "- **Numbers:** *Training(972)* - Gliomas: 647 Brain Mets: 325 *Validation(209)* - Gliomas:139 Brain Mets: 70 *Test(207)* - Gliomas:138 Brain Mets: 69\n",
    "- File *train.csv*, *val.csv* and *test.csv* was created which had class labels, image path, segmentation path and case_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92e593-e3ff-43bf-af5f-448b7a45f26f",
   "metadata": {},
   "source": [
    "## Importing Libraries used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f32b44-f4cd-4f9c-9389-41f85c414394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, json, random, csv, hashlib, platform, subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, decollate_batch\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandAffined,\n",
    "    AsDiscreted,\n",
    "    CastToTyped,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21017b85-13e6-4821-9058-6a790c463825",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1ef48-06f7-453f-bd4c-62b826a257c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_determinism(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "    except Exception:\n",
    "        gpu_name = \"Unknown CUDA device\"\n",
    "    print(f\"Device: {device} ({gpu_name})\")\n",
    "else:\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "# Paths\n",
    "PROJ_ROOT = Path(\"/home/ant/projects/brain_tumor_segmentation\")\n",
    "DUALTASK_ROOT = PROJ_ROOT / \"derived\" / \"unified_dualtask\"\n",
    "TRAIN_CSV = DUALTASK_ROOT / \"train.csv\"\n",
    "VAL_CSV = DUALTASK_ROOT / \"val.csv\"\n",
    "TEST_CSV = DUALTASK_ROOT / \"test.csv\"\n",
    "\n",
    "# Basic path checks with clear messages\n",
    "missing = [p for p in [DUALTASK_ROOT, TRAIN_CSV, VAL_CSV, TEST_CSV] if not p.exists()]\n",
    "assert not missing, f\"Missing required paths: {', '.join(str(p) for p in missing)}\"\n",
    "\n",
    "# Target spacing and patch params\n",
    "TARGET_SPACING = (0.8, 0.8, 1.0)\n",
    "PATCH_SIZE = (192, 192, 160)\n",
    "PATCH_OVERLAP = 0.5  # sliding window overlap\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = PROJ_ROOT / \"runs\" / \"dualtask_monai_v01\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoint dir:\", ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa449a-34b8-4b00-a035-850471a73398",
   "metadata": {},
   "source": [
    "## Run/Session paths and CSV helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305adb4a-0dff-4f4c-9c2e-cd21f9700aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_DIR = ckpt_dir / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Common artifact paths for this run\n",
    "METRICS_CSV = RUN_DIR / \"metrics.csv\"\n",
    "CONFIG_JSON = RUN_DIR / \"config.json\"\n",
    "ENV_JSON = RUN_DIR / \"env.json\"\n",
    "QC_CSV = RUN_DIR / \"qc_epoch_summary.csv\"\n",
    "\n",
    "def write_csv_header(path: Path, header: List[str]):\n",
    "    \"\"\"\n",
    "    Open a CSV for appending and write the header if the file doesn't exist yet.\n",
    "    Returns (file_handle, csv_writer). Caller is responsible for closing the file_handle.\n",
    "    \"\"\"\n",
    "    is_new = not path.exists()\n",
    "    f = open(path, \"a\", newline=\"\")\n",
    "    w = csv.writer(f)\n",
    "    if is_new:\n",
    "        w.writerow(header)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "    return f, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d07418-2dd7-4091-9223-19106f0546a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV → list[dict] helpers with validation\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "def read_unified_csv(path: Path, check_files: bool = True) -> List[Dict[str, Any]]:\n",
    "    df = pd.read_csv(path)\n",
    "    expected_cols = {\"case_id\", \"class_label\", \"image_path\", \"label_path\"}\n",
    "    missing = expected_cols - set(df.columns)\n",
    "    assert not missing, f\"Missing columns in {path.name}: {sorted(missing)}\"\n",
    "\n",
    "    # Clean and coerce types\n",
    "    df = df.copy()\n",
    "    df[\"case_id\"] = df[\"case_id\"].astype(str).str.strip()\n",
    "    df[\"image_path\"] = df[\"image_path\"].astype(str).str.strip()\n",
    "    df[\"label_path\"] = df[\"label_path\"].astype(str).str.strip()\n",
    "    df[\"class_label\"] = pd.to_numeric(df[\"class_label\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "    # Optional: drop rows whose files don’t exist\n",
    "    if check_files:\n",
    "        def exists(p: str) -> bool: return Path(p).exists()\n",
    "        bad_mask = (~df[\"image_path\"].map(exists)) | (~df[\"label_path\"].map(exists))\n",
    "        if bad_mask.any():\n",
    "            n_bad = int(bad_mask.sum())\n",
    "            print(f\"[WARN] {n_bad} rows dropped from {path.name} due to missing files\")\n",
    "            df = df.loc[~bad_mask]\n",
    "\n",
    "    # Build MONAI-style dicts\n",
    "    items: List[Dict[str, Any]] = [\n",
    "        {\n",
    "            \"case_id\": r[\"case_id\"],\n",
    "            \"image\": r[\"image_path\"],\n",
    "            \"label\": r[\"label_path\"],\n",
    "            \"class_label\": int(r[\"class_label\"]),\n",
    "        }\n",
    "        for _, r in df.iterrows()\n",
    "    ]\n",
    "    return items\n",
    "\n",
    "train_items = read_unified_csv(TRAIN_CSV)\n",
    "val_items = read_unified_csv(VAL_CSV)\n",
    "test_items = read_unified_csv(TEST_CSV)\n",
    "\n",
    "len(train_items), len(val_items), len(test_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dc2c7-338e-43a7-9756-2f773f3c68f6",
   "metadata": {},
   "source": [
    "## Morphology utilities and QC counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a579666-642f-4161-9bdb-27f1dec62e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndi\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "class LabelQC:\n",
    "    \"\"\"\n",
    "    Tracks label shrinkage across patches/cases.\n",
    "    - shrink_warn_threshold: fraction of volume lost (e.g., 0.35 → warn if after < 65% of before)\n",
    "    - verbose: if True, prints a line for each warning\n",
    "    \"\"\"\n",
    "    def __init__(self, shrink_warn_threshold: float = 0.35, verbose: bool = True):\n",
    "        self.shrink_warn_threshold = float(shrink_warn_threshold)\n",
    "        self.verbose = bool(verbose)\n",
    "        self.total: int = 0\n",
    "        self.warn: int = 0\n",
    "        self.flagged_examples: List[Tuple[str, int, int, float]] = []  # (case_id, before, after, ratio)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total = 0\n",
    "        self.warn = 0\n",
    "        self.flagged_examples.clear()\n",
    "\n",
    "    @property\n",
    "    def rate(self) -> float:\n",
    "        return self.warn / max(1, self.total)\n",
    "\n",
    "    def update(self, before_voxels: int, after_voxels: int, case_id: str) -> None:\n",
    "        self.total += 1\n",
    "        if before_voxels <= 0:\n",
    "            return\n",
    "        ratio = (after_voxels + 1e-6) / (before_voxels + 1e-6)\n",
    "        if ratio < (1.0 - self.shrink_warn_threshold):\n",
    "            self.warn += 1\n",
    "            self.flagged_examples.append((case_id, int(before_voxels), int(after_voxels), float(ratio)))\n",
    "            if self.verbose:\n",
    "                print(f\"[QC] label shrinkage: {case_id} before={before_voxels} after={after_voxels} ratio={ratio:.3f}\")\n",
    "\n",
    "    def summary(self) -> None:\n",
    "        print(f\"[QC] shrinkage warnings: {self.warn}/{self.total} (rate={self.rate:.4f})\")\n",
    "\n",
    "def binary_dilate_then_erode(mask: np.ndarray, radius_vox: int = 1, connectivity: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Light morphological close (dilate then erode) to protect thin/fragmented labels.\n",
    "    - mask: 3D array (D,H,W). Nonzeros are treated as foreground.\n",
    "    - radius_vox: number of iterations for dilation and erosion. 0 → no-op.\n",
    "    - connectivity: 1 (faces), 2 (faces+edges), or 3 (faces+edges+corners).\n",
    "    Returns uint8 mask (0/1).\n",
    "    \"\"\"\n",
    "    if radius_vox <= 0:\n",
    "        return (mask > 0).astype(np.uint8)\n",
    "    if mask.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D mask, got shape {mask.shape}\")\n",
    "\n",
    "    mask_bool = (mask > 0)\n",
    "    structure = ndi.generate_binary_structure(rank=3, connectivity=int(connectivity))\n",
    "    dil = ndi.binary_dilation(mask_bool, structure=structure, iterations=int(radius_vox))\n",
    "    ero = ndi.binary_erosion(dil, structure=structure, iterations=int(radius_vox))\n",
    "    return ero.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606db4a7-e45d-4102-be96-e883bbc7a216",
   "metadata": {},
   "source": [
    "## Transformation and Label Handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6efcbf-016f-4c2f-93eb-9c2743d78d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms: spacing standardization and intensity scale\n",
    "# Labels: post-process after Spacingd to preserve small lesions (no extra soft resampling)\n",
    "\n",
    "from monai.transforms import MapTransform\n",
    "\n",
    "class LabelPostProcessd(MapTransform):\n",
    "    \"\"\"\n",
    "    - Ensures label shape matches image shape (nearest-neighbor up/down-sample if needed)\n",
    "    - Optional light morphology (dilate→erode) to protect thin/speck lesions\n",
    "    - Records QC counts: qc_before_vox, qc_after_vox\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, ref_key: str = \"image\", morph_radius: int = 1, allow_missing_keys: bool = False):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.ref_key = ref_key\n",
    "        self.morph_radius = int(morph_radius)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        if \"label\" not in d:\n",
    "            return d\n",
    "\n",
    "        label = d[\"label\"]  # Tensor [1, D, H, W]\n",
    "        img = d.get(self.ref_key, None)\n",
    "        before_vox = int((label > 0).sum().item())\n",
    "\n",
    "        # Match label to image grid if needed (nearest to avoid smoothing)\n",
    "        if img is not None and label.shape[1:] != img.shape[1:]:\n",
    "            label = torch.nn.functional.interpolate(\n",
    "                label.float(), size=img.shape[1:], mode=\"nearest\"\n",
    "            ).long()\n",
    "\n",
    "        # Optional light morphology\n",
    "        if self.morph_radius > 0:\n",
    "            arr = (label > 0).cpu().numpy().astype(np.uint8)  # [1, D, H, W]\n",
    "            arr = binary_dilate_then_erode(arr[0], radius_vox=self.morph_radius)[None]  # back to [1, ...]\n",
    "            label = torch.as_tensor(arr, dtype=torch.long, device=d[\"label\"].device)\n",
    "\n",
    "        after_vox = int((label > 0).sum().item())\n",
    "\n",
    "        d[\"label\"] = label\n",
    "        d[\"qc_before_vox\"] = before_vox\n",
    "        d[\"qc_after_vox\"] = after_vox\n",
    "        return d\n",
    "\n",
    "\n",
    "# Common I/O and spacing (labels via nearest to avoid smoothing)\n",
    "common_load = [\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    Spacingd(keys=[\"image\", \"label\"], pixdim=TARGET_SPACING, mode=(\"bilinear\", \"nearest\")),\n",
    "]\n",
    "\n",
    "from monai.transforms import RandCropByPosNegLabeld, SpatialPadd\n",
    "\n",
    "# Training intensity + spatial augs\n",
    "intensity_train = [\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=3000, b_min=0.0, b_max=1.0, clip=True),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=[0, 1, 2], prob=0.2),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.2, max_k=3),\n",
    "    RandAffined(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        rotate_range=(math.pi/36, math.pi/36, math.pi/36),\n",
    "        scale_range=(0.1, 0.1, 0.1),\n",
    "        mode=(\"bilinear\", \"nearest\"),\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    SpatialPadd(keys=[\"image\", \"label\"], spatial_size=PATCH_SIZE),\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=PATCH_SIZE,\n",
    "        pos=1, neg=1, num_samples=1, image_key=\"image\",\n",
    "        allow_smaller=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Validation intensity only\n",
    "intensity_val = [\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=3000, b_min=0.0, b_max=1.0, clip=True),\n",
    "]\n",
    "\n",
    "# Cast classification label\n",
    "class CastClassLabeld(MapTransform):\n",
    "    def __init__(self, keys, allow_missing_keys=False):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        if \"class_label\" in d:\n",
    "            d[\"class_label\"] = torch.as_tensor(d[\"class_label\"], dtype=torch.float32)\n",
    "        return d\n",
    "\n",
    "# Assemble transforms\n",
    "# - Post label process uses nearest resample if shapes differ (no soft one-hot resample)\n",
    "# - Morph radius=1; raise to 2 if QC indicates too many tiny lesions vanish\n",
    "post_label_preserve = [LabelPostProcessd(keys=[\"label\"], ref_key=\"image\", morph_radius=1)]\n",
    "\n",
    "train_transforms = Compose(common_load + intensity_train + post_label_preserve + [CastClassLabeld(keys=[\"class_label\"])])\n",
    "val_transforms = Compose(common_load + intensity_val + post_label_preserve + [CastClassLabeld(keys=[\"class_label\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abc22f-20a7-451f-bd89-7595f3fa257c",
   "metadata": {},
   "source": [
    "## Datasets and Loaders (with reproducible workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48364ab7-812e-4c57-af75-cad1f36b53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC accumulators (patch-level stats already added in transforms and training loop)\n",
    "qc_train = LabelQC(shrink_warn_threshold=0.35, verbose=True)\n",
    "qc_val = LabelQC(shrink_warn_threshold=0.35, verbose=True)\n",
    "\n",
    "# Reproducible dataloader workers\n",
    "def seed_worker(worker_id: int):\n",
    "    base_seed = SEED\n",
    "    np.random.seed(base_seed + worker_id)\n",
    "    random.seed(base_seed + worker_id)\n",
    "    torch.manual_seed(base_seed + worker_id)\n",
    "\n",
    "use_pin = (device.type == \"cuda\")\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Datasets (cache_rate=0.0 to avoid RAM pressure; switch to >0.0 if you want caching)\n",
    "train_ds = CacheDataset(data=train_items, transform=train_transforms, cache_rate=0.0, num_workers=0)\n",
    "val_ds   = CacheDataset(data=val_items,   transform=val_transforms,   cache_rate=0.0, num_workers=0)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=use_pin,\n",
    "    persistent_workers=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=use_pin,\n",
    "    persistent_workers=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b3373-5177-46eb-8bd5-d1967c46cb5a",
   "metadata": {},
   "source": [
    "## Model: DynUNet backbone + classification head from encoder bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3b37d-fde5-42ba-959a-b6ac8323462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation network (nnU-Net-like DynUNet)\n",
    "seg_net = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    kernel_size=[3, 3, 3, 3, 3, 3],     # 6 stages\n",
    "    strides=[1, 2, 2, 2, 2, 2],         # length matches kernel_size\n",
    "    upsample_kernel_size=[2, 2, 2, 2, 2],\n",
    "    norm_name=\"instance\",\n",
    "    deep_supervision=False,\n",
    ").to(device)\n",
    "\n",
    "# Classification head (lazy: initializes fully-connected layer on first forward)\n",
    "class LazyClassificationHead(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, feat: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(feat).flatten(1)\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(x.shape[1], self.num_classes).to(x.device)\n",
    "        return self.fc(x)\n",
    "\n",
    "cls_head = LazyClassificationHead(num_classes=1).to(device)\n",
    "\n",
    "# Hook to capture encoder bottleneck features for classification\n",
    "encoder_feat = {\"x\": None}\n",
    "def hook_fn(module, input, output):\n",
    "    encoder_feat[\"x\"] = output\n",
    "\n",
    "# Attach hook to a stable location in DynUNet\n",
    "if hasattr(seg_net, \"bottleneck\"):\n",
    "    seg_net.bottleneck.register_forward_hook(hook_fn)\n",
    "elif hasattr(seg_net, \"encoder4\"):\n",
    "    seg_net.encoder4.register_forward_hook(hook_fn)\n",
    "else:\n",
    "    print(\"[WARN] Could not attach hook; classification head may not receive features\")\n",
    "\n",
    "# Losses and optimizer\n",
    "seg_loss_fn = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "cls_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "params = list(seg_net.parameters()) + list(cls_head.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "# AMP scaler\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Segmentation metrics\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e84eb-d9ce-4467-9182-f9eedff27c14",
   "metadata": {},
   "source": [
    "## Helper: pad tensor to next multiple-of factor for each spatial dim (+ safe crop helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701d691-731f-4b5f-91ae-9500be5dc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_factor(x: torch.Tensor, factor=32, return_pad: bool = False, mode: str = \"constant\", value: float = 0.0):\n",
    "    \"\"\"\n",
    "    Pads a 5D tensor (B, C, D, H, W) so each spatial dim is a multiple of `factor`.\n",
    "    - factor: int or (fD, fH, fW)\n",
    "    - Pads only on the \"right/bottom/back\" to avoid shifting coordinates\n",
    "    - If return_pad=True, also returns the pad tuple (Wl, Wr, Hl, Hr, Dl, Dr)\n",
    "    \"\"\"\n",
    "    assert x.dim() == 5, f\"Expected 5D tensor (B,C,D,H,W), got shape {tuple(x.shape)}\"\n",
    "    if isinstance(factor, int):\n",
    "        fD = fH = fW = factor\n",
    "    else:\n",
    "        assert len(factor) == 3, \"factor must be int or 3-tuple\"\n",
    "        fD, fH, fW = factor\n",
    "\n",
    "    B, C, D, H, W = x.shape\n",
    "    def next_m(s, f): return ((s + f - 1) // f) * f\n",
    "    Dn, Hn, Wn = next_m(D, fD), next_m(H, fH), next_m(W, fW)\n",
    "    pd, ph, pw = Dn - D, Hn - H, Wn - W\n",
    "    pad = (0, pw, 0, ph, 0, pd)  # (W_left, W_right, H_left, H_right, D_left, D_right)\n",
    "\n",
    "    if any(p > 0 for p in pad):\n",
    "        x = F.pad(x, pad, mode=mode, value=value)\n",
    "\n",
    "    return (x, pad) if return_pad else x\n",
    "\n",
    "def crop_to_shape(x: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crops tensor x (B,C,D,H,W) to spatial shape `shape` = (D,H,W), slicing from the start along each dim.\n",
    "    \"\"\"\n",
    "    assert x.dim() == 5 and len(shape) == 3, \"x must be 5D and shape must be (D,H,W)\"\n",
    "    Dz, Hy, Wx = map(int, shape)\n",
    "    return x[..., :Dz, :Hy, :Wx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc41140-f36c-4733-b180-d08ae9414a73",
   "metadata": {},
   "source": [
    "## Inferer for validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f099da9-afef-45b9-b733-ab4c40a076fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferer = SlidingWindowInferer(\n",
    "    roi_size=PATCH_SIZE,\n",
    "    sw_batch_size=1,\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    mode=\"gaussian\",\n",
    ")\n",
    "\n",
    "# Utils\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "def to_device(batch: Dict[str, Any], device: torch.device) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            out[k] = v.to(device, non_blocking=True)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def sliding_infer_padded(\n",
    "    images: torch.Tensor,\n",
    "    network: nn.Module,\n",
    "    factor: int = 32,\n",
    "    target_shape: Optional[Tuple[int, int, int]] = None,\n",
    ") -> torch.Tensor:\n",
    "    images_p, _ = pad_to_factor(images, factor=factor, return_pad=True)\n",
    "    logits = inferer(inputs=images_p, network=network)\n",
    "    if target_shape is not None and logits.shape[-3:] != tuple(target_shape):\n",
    "        logits = crop_to_shape(logits, target_shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944918a-0134-4f7e-8b62-44729389dab3",
   "metadata": {},
   "source": [
    "## Setting up training configuration and saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8ded9-fb10-431f-b8ea-749fd3a12583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_sha1(path: Path | str) -> str | None:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return hashlib.sha1(f.read()).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "config = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"gpu_name\": (torch.cuda.get_device_name(0) if torch.cuda.is_available() else None),\n",
    "    \"target_spacing\": tuple(TARGET_SPACING),\n",
    "    \"patch_size\": tuple(PATCH_SIZE),\n",
    "    \"inferer\": {\"roi_size\": tuple(PATCH_SIZE), \"overlap\": float(PATCH_OVERLAP), \"mode\": \"gaussian\"},\n",
    "    \"model\": {\n",
    "        \"arch\": \"DynUNet\",\n",
    "        \"in_channels\": 1,\n",
    "        \"out_channels\": 2,\n",
    "        \"deep_supervision\": False,\n",
    "        \"norm_name\": \"instance\",\n",
    "    },\n",
    "    \"loss\": {\"seg\": \"DiceCELoss\", \"cls\": \"BCEWithLogitsLoss\", \"cls_weight\": 0.3},\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "        \"weight_decay\": float(optimizer.param_groups[0].get(\"weight_decay\", 0.0)),\n",
    "    },\n",
    "    \"augs\": {\n",
    "        \"flip_p\": 0.2, \"rot90_p\": 0.2,\n",
    "        \"affine\": {\"rot_deg\": 5, \"scale_pct\": 10},\n",
    "        \"crop\": \"RandCropByPosNegLabel\",\n",
    "    },\n",
    "    # Reflect current pipeline (nearest for labels + light morph).\n",
    "    \"transforms_notes\": \"Spacingd (image=bilinear, label=nearest), LabelPostProcessd(morph_radius=1)\",\n",
    "    \"splits\": {\n",
    "        \"train_csv\": str(TRAIN_CSV), \"val_csv\": str(VAL_CSV), \"test_csv\": str(TEST_CSV),\n",
    "        \"train_csv_sha1\": file_sha1(TRAIN_CSV), \"val_csv_sha1\": file_sha1(VAL_CSV), \"test_csv_sha1\": file_sha1(TEST_CSV),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(CONFIG_JSON, \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Saved config:\", CONFIG_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f0504-21be-47eb-b663-197dec99b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment snapshot (for full reproducibility)\n",
    "env = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"os\": platform.platform(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"torch_cuda\": (torch.version.cuda if torch.cuda.is_available() else None),\n",
    "    \"cudnn\": (torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None),\n",
    "    \"monai\": __import__(\"monai\").__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"gpu_count\": (torch.cuda.device_count() if torch.cuda.is_available() else 0),\n",
    "    \"gpu_name\": (torch.cuda.get_device_name(0) if torch.cuda.is_available() else None),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "# List all GPU names if multiple are present\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    env[\"gpu_names\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "\n",
    "# Optional: exact package set (can be large)\n",
    "try:\n",
    "    env[\"pip_freeze\"] = subprocess.check_output([\"pip\", \"freeze\"]).decode().splitlines()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "with open(ENV_JSON, \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "print(\"Saved env:\", ENV_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18fcd2-06f7-4e25-8a18-e7b8dd3f0fd7",
   "metadata": {},
   "source": [
    "## Resume Training: prefer most recent \"last.pt\", otherwise fall back to \"best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f98b5-11da-4cba-ab68-1f6fc4b9b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_resume_ckpt() -> Path:\n",
    "    candidates = []\n",
    "    # Prefer run-scoped last/best if present\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        candidates += [RUN_DIR / \"last.pt\", RUN_DIR / \"best.pt\"]\n",
    "    # Fallback to global dir\n",
    "    candidates += [ckpt_dir / \"last.pt\", ckpt_dir / \"best.pt\"]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No checkpoint found in {ckpt_dir} (looked for last.pt/best.pt in run/global dirs).\")\n",
    "\n",
    "RESUME_CKPT = find_resume_ckpt()\n",
    "print(f\"Resuming from: {RESUME_CKPT}\")\n",
    "\n",
    "# Initialize lazy cls_head.fc before loading (captures encoder channels safely)\n",
    "with torch.no_grad():\n",
    "    was_training = seg_net.training\n",
    "    seg_net.eval()\n",
    "    encoder_feat[\"x\"] = None\n",
    "    dummy = torch.zeros(1, 1, 64, 64, 64, device=device)\n",
    "    _ = seg_net(dummy)\n",
    "    feat = encoder_feat[\"x\"] if encoder_feat[\"x\"] is not None else dummy\n",
    "    _ = cls_head(feat)  # initializes cls_head.fc lazily\n",
    "    seg_net.train(was_training)\n",
    "\n",
    "ckpt = torch.load(RESUME_CKPT, map_location=device)\n",
    "\n",
    "missing, unexpected = seg_net.load_state_dict(ckpt[\"seg\"], strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] seg_net state_dict mismatches. missing={missing}, unexpected={unexpected}\")\n",
    "missing, unexpected = cls_head.load_state_dict(ckpt[\"cls\"], strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] cls_head state_dict mismatches. missing={missing}, unexpected={unexpected}\")\n",
    "\n",
    "if \"optimizer\" in ckpt:\n",
    "    try:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] could not load optimizer state: {e}\")\n",
    "if \"scaler\" in ckpt:\n",
    "    try:\n",
    "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] could not load scaler state: {e}\")\n",
    "\n",
    "# Epoch planning\n",
    "start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "best_val_dice = float(ckpt.get(\"best_val_dice\", -1.0))\n",
    "EPOCHS_NEXT = 50\n",
    "end_epoch = start_epoch + EPOCHS_NEXT - 1\n",
    "\n",
    "print(f\"Resuming at epoch {start_epoch} (best_val_dice={best_val_dice:.4f}); training until epoch {end_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1728d-f173-4dd8-84da-db2fc1f1fccd",
   "metadata": {},
   "source": [
    "### Open metrics CSV (segmentation + training stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2452e-adff-44bb-8605-9d4ef3f62ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open metrics CSV (segmentation + training stats)\n",
    "# Close with: metrics_f.close() at the end of training\n",
    "metrics_f, metrics_w = write_csv_header(\n",
    "    METRICS_CSV,\n",
    "    [\"epoch\",\"split\",\"loss\",\"dice\",\"acc\",\"recall\",\"f1\",\"hd95\",\"lr\",\"seconds\",\"qc_warns\",\"qc_total\",\"qc_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ba1d8-8aba-401a-9d1b-8304623c2fb6",
   "metadata": {},
   "source": [
    "## Segmentation metric helpers (To be used with training/validation loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e41f33-9141-4a24-9675-94374fd854db",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "hd95_tr = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile=95)\n",
    "hd95_val = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile=95)\n",
    "\n",
    "def new_cm():\n",
    "    return {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "\n",
    "def update_cm(cm, ypred, ytrue):\n",
    "    yp = (ypred > 0).to(torch.bool)\n",
    "    yt = (ytrue > 0).to(torch.bool)\n",
    "    cm[\"tp\"] += (yp & yt).sum().item()\n",
    "    cm[\"fp\"] += (yp & ~yt).sum().item()\n",
    "    cm[\"fn\"] += (~yp & yt).sum().item()\n",
    "    cm[\"tn\"] += (~yp & ~yt).sum().item()\n",
    "\n",
    "def cm_metrics(cm):\n",
    "    tp, fp, fn, tn = cm[\"tp\"], cm[\"fp\"], cm[\"fn\"], cm[\"tn\"]\n",
    "    acc = (tp + tn) / max(1, tp + fp + fn + tn)\n",
    "    rec = tp / max(1, tp + fn)\n",
    "    pre = tp / max(1, tp + fp)\n",
    "    f1 = (2 * pre * rec) / max(eps, pre + rec)\n",
    "    return acc, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aaa60d-1844-48c4-893d-9a941151574a",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d12f12-067c-4511-a64d-58fb659ad3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val loops (refactored: correct metrics, single CSV writer, stable val Dice)\n",
    "\n",
    "# Assumes:\n",
    "# - start_epoch, end_epoch, best_val_dice defined (from resume block); if not, set them here.\n",
    "# - metrics_f, metrics_w already opened by write_csv_header with header:\n",
    "#   [\"epoch\",\"split\",\"loss\",\"dice\",\"acc\",\"recall\",\"f1\",\"hd95\",\"lr\",\"seconds\",\"qc_warns\",\"qc_total\",\"qc_rate\"]\n",
    "\n",
    "if \"start_epoch\" not in globals() or \"end_epoch\" not in globals():\n",
    "    EPOCHS = 500\n",
    "    start_epoch, end_epoch = 1, EPOCHS\n",
    "if \"best_val_dice\" not in globals():\n",
    "    best_val_dice = -1.0\n",
    "\n",
    "val_interval = 1\n",
    "\n",
    "t0_all = time.time()\n",
    "for epoch in range(start_epoch, end_epoch + 1):\n",
    "    t_epoch = time.time()\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    cm_train = new_cm()\n",
    "    seg_net.train(); cls_head.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_steps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # QC update per-sample (patch-level)\n",
    "        for b in decollate_batch(batch):\n",
    "            qc_train.update(int(b.get(\"qc_before_vox\", 0)), int(b.get(\"qc_after_vox\", 0)), str(b.get(\"case_id\", \"?\")))\n",
    "        batch = to_device(batch, device)\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"].long()\n",
    "        class_labels = batch[\"class_label\"].view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
    "            # segmentation forward\n",
    "            encoder_feat[\"x\"] = None\n",
    "            seg_logits = seg_net(images)                 # deep_supervision=False\n",
    "            seg_logits_main = seg_logits\n",
    "\n",
    "            # train segmentation metrics (per-batch)\n",
    "            y_pred_tr = torch.argmax(torch.softmax(seg_logits_main, dim=1), dim=1, keepdim=True)\n",
    "            update_cm(cm_train, y_pred_tr, labels)\n",
    "            hd95_tr(y_pred_tr, labels)\n",
    "\n",
    "            # classification forward\n",
    "            feat = encoder_feat[\"x\"] if encoder_feat[\"x\"] is not None else seg_logits_main\n",
    "            cls_logits = cls_head(feat)\n",
    "\n",
    "            # losses\n",
    "            loss_seg = seg_loss_fn(seg_logits_main, labels)\n",
    "            loss_cls = cls_loss_fn(cls_logits, class_labels)\n",
    "            loss = loss_seg + 0.3 * loss_cls\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_steps += 1\n",
    "\n",
    "    epoch_loss /= max(1, num_steps)\n",
    "    tr_acc, tr_rec, tr_f1 = cm_metrics(cm_train)\n",
    "    tr_hd95 = hd95_tr.aggregate().item(); hd95_tr.reset()\n",
    "    train_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    qc_rate_tr = (qc_train.warn / max(1, qc_train.total))\n",
    "\n",
    "    print(f\"Epoch {epoch}/{end_epoch} - train loss: {epoch_loss:.4f}\")\n",
    "    print(f\"  Train: acc={tr_acc:.4f} rec={tr_rec:.4f} f1={tr_f1:.4f} hd95={tr_hd95:.2f}\")\n",
    "    metrics_w.writerow([\n",
    "        epoch, \"train\",\n",
    "        f\"{epoch_loss:.6f}\", \"\", f\"{tr_acc:.6f}\", f\"{tr_rec:.6f}\", f\"{tr_f1:.6f}\", f\"{tr_hd95:.6f}\",\n",
    "        f\"{train_lr:.6g}\", f\"{(time.time()-t_epoch):.2f}\", qc_train.warn, qc_train.total, f\"{qc_rate_tr:.4f}\"\n",
    "    ]); metrics_f.flush()\n",
    "\n",
    "    # reset QC per-epoch if you prefer epoch-local stats\n",
    "    qc_train.total = qc_train.warn = 0\n",
    "    if epoch % val_interval == 0:\n",
    "        qc_train.summary()\n",
    "\n",
    "    # ---- VAL ----\n",
    "    if epoch % val_interval == 0:\n",
    "        cm_val = new_cm()\n",
    "        seg_net.eval(); cls_head.eval()\n",
    "        dice_metric.reset()\n",
    "        val_loss = 0.0\n",
    "        steps = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for b in decollate_batch(batch):\n",
    "                    qc_val.update(int(b.get(\"qc_before_vox\", 0)), int(b.get(\"qc_after_vox\", 0)), str(b.get(\"case_id\", \"?\")))\n",
    "                batch = to_device(batch, device)\n",
    "                images = batch[\"image\"]\n",
    "                labels = batch[\"label\"].long()\n",
    "                class_labels = batch[\"class_label\"].view(-1, 1)\n",
    "\n",
    "                with autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
    "                    # padded sliding-window seg\n",
    "                    images_p = pad_to_factor(images, factor=32)\n",
    "                    seg_logits = inferer(inputs=images_p, network=seg_net)\n",
    "\n",
    "                    # classification (populate encoder features on same padded grid)\n",
    "                    encoder_feat[\"x\"] = None\n",
    "                    _ = seg_net(images_p)\n",
    "                    if seg_logits.shape[-3:] != labels.shape[-3:]:\n",
    "                        seg_logits = crop_to_shape(seg_logits, labels.shape[-3:])\n",
    "\n",
    "                    feat = encoder_feat[\"x\"] if encoder_feat[\"x\"] is not None else seg_logits\n",
    "                    cls_logits = cls_head(feat)\n",
    "\n",
    "                    # per-batch seg metrics\n",
    "                    y_pred = torch.argmax(torch.softmax(seg_logits, dim=1), dim=1, keepdim=True)\n",
    "                    dice_metric(y_pred=y_pred, y=labels)\n",
    "                    update_cm(cm_val, y_pred, labels)\n",
    "                    hd95_val(y_pred, labels)\n",
    "\n",
    "                    # per-batch val loss\n",
    "                    loss_seg = seg_loss_fn(seg_logits, labels)\n",
    "                    loss_cls = cls_loss_fn(cls_logits, class_labels)\n",
    "                    loss = loss_seg + 0.3 * loss_cls\n",
    "                val_loss += loss.item()\n",
    "                steps += 1\n",
    "\n",
    "        mean_dice = dice_metric.aggregate().item(); dice_metric.reset()\n",
    "        val_loss /= max(1, steps)\n",
    "        vl_acc, vl_rec, vl_f1 = cm_metrics(cm_val)\n",
    "        vl_hd95 = hd95_val.aggregate().item(); hd95_val.reset()\n",
    "        qc_rate_val = (qc_val.warn / max(1, qc_val.total))\n",
    "\n",
    "        print(f\"  Val loss: {val_loss:.4f} | Val Dice(tumor): {mean_dice:.4f}\")\n",
    "        print(f\"  Val  : acc={vl_acc:.4f} rec={vl_rec:.4f} f1={vl_f1:.4f} hd95={vl_hd95:.2f}\")\n",
    "        qc_val.summary()\n",
    "\n",
    "        metrics_w.writerow([\n",
    "            epoch, \"val\",\n",
    "            f\"{val_loss:.6f}\", f\"{mean_dice:.6f}\", f\"{vl_acc:.6f}\", f\"{vl_rec:.6f}\", f\"{vl_f1:.6f}\", f\"{vl_hd95:.6f}\",\n",
    "            f\"{train_lr:.6g}\", f\"{(time.time()-t_epoch):.2f}\", qc_val.warn, qc_val.total, f\"{qc_rate_val:.4f}\"\n",
    "        ]); metrics_f.flush()\n",
    "        qc_val.total = qc_val.warn = 0\n",
    "\n",
    "        # ---- Checkpointing ----\n",
    "        def save_ckpt(path: Path):\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"seg\": seg_net.state_dict(),\n",
    "                \"cls\": cls_head.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scaler\": scaler.state_dict(),\n",
    "                \"best_val_dice\": best_val_dice,\n",
    "                \"config_path\": str(CONFIG_JSON) if \"CONFIG_JSON\" in globals() else None,\n",
    "                \"env_path\": str(ENV_JSON) if \"ENV_JSON\" in globals() else None,\n",
    "            }, path)\n",
    "\n",
    "        run_last = (RUN_DIR if \"RUN_DIR\" in globals() else ckpt_dir) / \"last.pt\"\n",
    "        save_ckpt(run_last)\n",
    "\n",
    "        if mean_dice > best_val_dice:\n",
    "            best_val_dice = mean_dice\n",
    "            run_best = (RUN_DIR if \"RUN_DIR\" in globals() else ckpt_dir) / \"best.pt\"\n",
    "            save_ckpt(run_best)\n",
    "            # convenience copy\n",
    "            save_ckpt(ckpt_dir / \"best.pt\")\n",
    "            print(f\"  [Saved] best.pt with Dice {best_val_dice:.4f}\")\n",
    "\n",
    "        # encoder-only (save full state; downstream can load encoder subset)\n",
    "        torch.save({\"seg_encoder_compatible\": True, \"state_dict\": seg_net.state_dict()},\n",
    "                   (RUN_DIR if \"RUN_DIR\" in globals() else ckpt_dir) / \"encoder_fullstate.pt\")\n",
    "\n",
    "        # Cleanup\n",
    "        try:\n",
    "            del images, labels, seg_logits, cls_logits, feat, y_pred\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del images_p\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Training done in {(time.time()-t0_all)/60:.1f} min\")\n",
    "# Close the metrics file (kept open for speed)\n",
    "try:\n",
    "    metrics_f.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc30a8-7c84-4425-809f-ae7490f0fc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fe8f0-696c-46a7-9732-73cd7fdca965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b4bd0-a042-491b-bfd6-fb3a1adfe03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee24f60-3f43-40d0-b13c-a4af7ff9f9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44802171-9ab8-4e46-b0ca-f33face35c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a96009-2bd2-4cab-806f-1355e6c06530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb110d-98d5-48d7-a9bf-dc9cb7938994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab079a5-6d21-4bcc-8048-77ae56679f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d12997-0d27-4e54-8b8c-7e0ae90bc1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34372185-ffd9-4437-8754-51a2ba6055e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38b77d-f1cf-44d0-8632-88c26160fdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87cbd4-bda6-4e76-8fe1-13e39b626513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dbda3-5ac2-472f-b0a7-5b444730f107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707c157-5c12-4fa5-9c91-cbe21a7c8137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e7927-a39c-4833-a570-abbb60f162eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa65f6f-e6f8-4d58-b019-17e3c6de9196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad969a-d726-492c-82ca-8ee38baa325b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
