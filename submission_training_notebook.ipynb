{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af652a0-98a7-494b-9021-6deab2487371",
   "metadata": {},
   "source": [
    "# Project: Brain Tumor Segmentation and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a3a1c-f95f-4409-8d1d-bfb4a24e96c6",
   "metadata": {},
   "source": [
    "## Details of Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c40a1a-ba59-4cba-b244-9d416297fcbb",
   "metadata": {},
   "source": [
    "#### [Data Source - The Cancer Imaging Archive - TCIA -- CLICK HERE](https://www.cancerimagingarchive.net/browse-collections)\n",
    "#### [Data prepration and full training and evaluation code -- CLICK HERE](https://github.com/kundan1974/DHAI-Brain-Segmentation)\n",
    "- **UCSF-PGDM:** Glioblastoma - 495\n",
    "- **BRATS-AFRICA:** Glioma - 95\n",
    "- **MU-Glioma-Post:** Glioma - 203\n",
    "- **UCSD-PTGBM:** Glioblastoma - 178\n",
    "- **UPENN-GBM:** Glioblastoma - 630\n",
    "- **BCBM-RadioGenomics:** Brain Mets - 165\n",
    "- **Pretreat-MetsToBrain-Masks:** Brain Mets - 200\n",
    "\n",
    "### Segmentation Dataset\n",
    "- **Numbers:** *495+95+203+178+630+165+200 = 1966*. But desired segmentation along with desired MRI sequence was present for - **1388** \n",
    "- **Segmentation:** Tumor core plus Enhancing area, Single segmentation mask and where two masks were provided like tumor core and enhancing area, then both the mask was combined and a single mask was derived - tumor with enhnacing area\n",
    "- **MRI sequence** used was - *T1 Contrast*\n",
    "- File *dataset.json* was created - Details of file path for MRI sequence and Segmentation and other dataset details\n",
    "\n",
    "### Classification dataset\n",
    "- **Numbers:** *Training(972)* - Gliomas: 647 Brain Mets: 325 *Validation(209)* - Gliomas:139 Brain Mets: 70 *Test(207)* - Gliomas:138 Brain Mets: 69\n",
    "- File *train.csv*, *val.csv* and *test.csv* was created which had class labels, image path, segmentation path and case_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad41cea-03a7-4b17-985a-0491eb6160a3",
   "metadata": {},
   "source": [
    "## üîß Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92e593-e3ff-43bf-af5f-448b7a45f26f",
   "metadata": {},
   "source": [
    "This cell imports all the required **standard libraries**, **PyTorch modules**, and **MONAI components** for our brain tumor segmentation and classification pipeline. Here's a breakdown:\n",
    "\n",
    "### üì¶ Standard Python & OS Libraries\n",
    "- `os`, `math`, `time`, `json`, `random`, `csv`, `hashlib`, `platform`, `subprocess`: Used for file operations, randomization, timing, system information, and scripting utilities.\n",
    "- `datetime`, `pathlib.Path`: Helpful for managing timestamps and path structures in a platform-independent way.\n",
    "- `typing`: Provides type hints for better code clarity and development support.\n",
    "\n",
    "### üî¢ Numerical & Data Libraries\n",
    "- `numpy`, `pandas`: Essential for handling numerical arrays and structured data tables respectively.\n",
    "\n",
    "### üî• PyTorch Core\n",
    "- `torch`, `torch.nn`, `torch.nn.functional`: For building, training, and evaluating deep learning models.\n",
    "- `torch.amp.autocast`, `GradScaler`: For enabling mixed precision training to speed up training and reduce GPU memory usage.\n",
    "\n",
    "### üß† Medical Imaging and Segmentation Tools (MONAI)\n",
    "- `nibabel`: For loading and working with medical image formats like NIfTI (`.nii`, `.nii.gz`).\n",
    "- `monai.config.print_config()`: Prints MONAI, PyTorch, and environment versions for reproducibility.\n",
    "- `monai.data`: Includes `CacheDataset` and `DataLoader` for efficient data handling and batch loading.\n",
    "- `monai.inferers.SlidingWindowInferer`: Used for patch-wise inference on large 3D volumes.\n",
    "- `monai.losses.DiceCELoss`: Hybrid Dice + CrossEntropy loss commonly used in segmentation tasks.\n",
    "- `monai.metrics`: Includes metrics such as Dice and Hausdorff for evaluating segmentation accuracy.\n",
    "- `monai.networks.nets.DynUNet`: A dynamic, flexible 3D U-Net model used for medical segmentation.\n",
    "- `monai.transforms`: Provides various preprocessing and augmentation transforms specifically tailored for medical image analysis.\n",
    "- `monai.utils.set_determinism`: Ensures reproducible results by fixing seeds and deterministic behavior.\n",
    "\n",
    "### ‚úÖ Purpose\n",
    "This cell sets the stage for all subsequent steps ‚Äî including data preparation, model building, training, evaluation, and inference ‚Äî by importing all dependencies and printing the current MONAI + PyTorch environment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f32b44-f4cd-4f9c-9389-41f85c414394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, json, random, csv, hashlib, platform, subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Optional\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, decollate_batch\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandAffined,\n",
    "    AsDiscreted,\n",
    "    CastToTyped,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21017b85-13e6-4821-9058-6a790c463825",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Reproducibility, Device Setup, and Configuration Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a6db7-6182-48c5-9e8e-3a8b337639c9",
   "metadata": {},
   "source": [
    "This cell establishes essential configurations to ensure reproducibility, select compute device (GPU/CPU), and define file paths and model parameters.\n",
    "\n",
    "### üîÅ Reproducibility\n",
    "- `SEED = 42` sets a fixed seed for random number generation.\n",
    "- `set_determinism(SEED)` ensures reproducibility by:\n",
    "  - Seeding all necessary libraries (NumPy, PyTorch, etc.)\n",
    "  - Enabling deterministic behavior for CUDA operations (when applicable).\n",
    "\n",
    "### üíª Device Selection\n",
    "- `torch.device(...)` selects GPU (`cuda`) if available, otherwise defaults to CPU.\n",
    "- If using CUDA, it prints the name of the GPU (e.g., ‚ÄúNVIDIA A6000‚Äù).\n",
    "- This helps verify that model training/inference will utilize available hardware acceleration.\n",
    "\n",
    "### üìÇ Dataset and Project Paths\n",
    "- `PROJ_ROOT`: Root directory of the project on the local system.\n",
    "- `DUALTASK_ROOT`: Subdirectory that contains derived data for dual-task learning (segmentation + classification).\n",
    "- `TRAIN_CSV`, `VAL_CSV`, `TEST_CSV`: Point to training, validation, and test CSV files, respectively.\n",
    "- An assertion ensures all these paths exist before proceeding.\n",
    "\n",
    "### üßä Spatial and Patch Parameters\n",
    "- `TARGET_SPACING`: The voxel spacing to which all MRI volumes will be resampled. Standardizing spacing is crucial for 3D medical image processing.\n",
    "- `PATCH_SIZE`: Size of the 3D patch that will be extracted from each image for model training and inference. Used by the sliding window.\n",
    "- `PATCH_OVERLAP`: Defines the overlap fraction between adjacent patches during inference (0.5 = 50% overlap).\n",
    "\n",
    "### üíæ Checkpoint Directory\n",
    "- `ckpt_dir`: Directory to save model weights and logs.\n",
    "- `mkdir(..., exist_ok=True)` creates the directory if it doesn't already exist.\n",
    "- Path is printed to confirm where checkpoints will be stored during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1ef48-06f7-453f-bd4c-62b826a257c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_determinism(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "    except Exception:\n",
    "        gpu_name = \"Unknown CUDA device\"\n",
    "    print(f\"Device: {device} ({gpu_name})\")\n",
    "else:\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "# Paths\n",
    "PROJ_ROOT = Path(\"/home/ant/projects/brain_tumor_segmentation\")\n",
    "DUALTASK_ROOT = PROJ_ROOT / \"derived\" / \"unified_dualtask\"\n",
    "TRAIN_CSV = DUALTASK_ROOT / \"train.csv\"\n",
    "VAL_CSV = DUALTASK_ROOT / \"val.csv\"\n",
    "TEST_CSV = DUALTASK_ROOT / \"test.csv\"\n",
    "\n",
    "# Basic path checks \n",
    "missing = [p for p in [DUALTASK_ROOT, TRAIN_CSV, VAL_CSV, TEST_CSV] if not p.exists()]\n",
    "assert not missing, f\"Missing required paths: {', '.join(str(p) for p in missing)}\"\n",
    "\n",
    "# Target spacing and patch params\n",
    "TARGET_SPACING = (0.8, 0.8, 1.0)\n",
    "PATCH_SIZE = (192, 192, 160)\n",
    "PATCH_OVERLAP = 0.5  # sliding window overlap\n",
    "\n",
    "# Checkpoint directory\n",
    "ckpt_dir = PROJ_ROOT / \"runs\" / \"dualtask_monai_v01\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoint dir:\", ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa449a-34b8-4b00-a035-850471a73398",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Run-Specific Directory & Logging Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11519031-4843-48e3-b1b1-7b47e04aabcc",
   "metadata": {},
   "source": [
    "This cell sets up unique directories and file paths for the current training or inference run, allowing us to store and track artifacts in an organized way.\n",
    "\n",
    "### üÜî Unique Run ID & Directory\n",
    "- `RUN_ID`: A timestamp-based string (e.g., `\"20250907-103845\"`) that uniquely identifies each run.\n",
    "- `RUN_DIR`: Subfolder under the checkpoint directory (`ckpt_dir`) specific to this run.\n",
    "- `mkdir(..., exist_ok=True)`: Ensures the folder is created even if it already exists.\n",
    "\n",
    "### üìÅ Paths for Artifacts\n",
    "Within `RUN_DIR`, we predefine the following key output file paths:\n",
    "- `METRICS_CSV`: To store evaluation metrics per epoch (e.g., Dice score, loss, etc.)\n",
    "- `CONFIG_JSON`: To store the configuration parameters used during this run.\n",
    "- `ENV_JSON`: To log software and hardware environment info (e.g., PyTorch/MONAI version).\n",
    "- `QC_CSV`: To summarize quality control or performance across epochs.\n",
    "\n",
    "These logs help with reproducibility, analysis, and comparison of different experiments.\n",
    "\n",
    "### üìù CSV Utility Function\n",
    "The `write_csv_header()` function:\n",
    "- Opens a CSV file in append mode.\n",
    "- Writes the header row only if the file doesn't already exist.\n",
    "- Flushes and syncs the file immediately to ensure safe write.\n",
    "- Returns the file handle and CSV writer object so that the caller can continue logging rows and close it when done.\n",
    "\n",
    "This is useful for writing structured logs (like metrics or quality control summaries) across multiple epochs or batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305adb4a-0dff-4f4c-9c2e-cd21f9700aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_DIR = ckpt_dir / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Common artifact paths for this run\n",
    "METRICS_CSV = RUN_DIR / \"metrics.csv\"\n",
    "CONFIG_JSON = RUN_DIR / \"config.json\"\n",
    "ENV_JSON = RUN_DIR / \"env.json\"\n",
    "QC_CSV = RUN_DIR / \"qc_epoch_summary.csv\"\n",
    "\n",
    "def write_csv_header(path: Path, header: List[str]):\n",
    "    \"\"\"\n",
    "    Open a CSV for appending and write the header if the file doesn't exist yet.\n",
    "    Returns (file_handle, csv_writer). Caller is responsible for closing the file_handle.\n",
    "    \"\"\"\n",
    "    is_new = not path.exists()\n",
    "    f = open(path, \"a\", newline=\"\")\n",
    "    w = csv.writer(f)\n",
    "    if is_new:\n",
    "        w.writerow(header)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "    return f, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d34766-f1db-4210-a59e-12c8ce727524",
   "metadata": {},
   "source": [
    "## üìÑ CSV ‚Üí List[Dict] Conversion with Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf206d9-1ddd-4c09-a994-549b886748c8",
   "metadata": {},
   "source": [
    "This cell defines a helper function to **read and validate dataset CSV files** (train/val/test), ensuring that each entry is correctly formatted and usable in the MONAI pipeline.\n",
    "\n",
    "### üß† Function: `read_unified_csv(path, check_files=True)`\n",
    "Reads a CSV (e.g., `train.csv`, `val.csv`, `test.csv`) and converts each row into a dictionary compatible with MONAI‚Äôs `Dataset` format. Key components:\n",
    "\n",
    "#### ‚úÖ Validation\n",
    "- Checks for required columns: `case_id`, `class_label`, `image_path`, `label_path`.\n",
    "- Raises an error if any expected column is missing.\n",
    "\n",
    "#### üßπ Cleaning & Type Coercion\n",
    "- Trims whitespace from paths and IDs.\n",
    "- Ensures `class_label` is a clean integer (invalid/missing values are coerced to `-1`).\n",
    "\n",
    "#### ‚ùå Optional File Existence Check\n",
    "- If `check_files=True`, verifies that both `image_path` and `label_path` point to valid files.\n",
    "- Drops any rows with missing files and prints a warning showing how many were removed.\n",
    "\n",
    "#### üîÅ Output Format\n",
    "- Returns a `List[Dict]`, where each dictionary contains:\n",
    "  - `\"case_id\"` ‚Äì Unique identifier for the subject/case\n",
    "  - `\"image\"` ‚Äì Path to the NIfTI image file\n",
    "  - `\"label\"` ‚Äì Path to the segmentation label file\n",
    "  - `\"class_label\"` ‚Äì Integer label (e.g., for classification: 0 = benign, 1 = malignant)\n",
    "\n",
    "This format is compatible with `monai.data.CacheDataset`.\n",
    "\n",
    "### üì¶ Dataset Preparation\n",
    "- `train_items`, `val_items`, and `test_items` are populated by calling `read_unified_csv(...)` for each respective CSV file.\n",
    "- The final line outputs the number of valid samples in each split as a tuple:  \n",
    "  `(num_train, num_val, num_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d07418-2dd7-4091-9223-19106f0546a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unified_csv(path: Path, check_files: bool = True) -> List[Dict[str, Any]]:\n",
    "    df = pd.read_csv(path)\n",
    "    expected_cols = {\"case_id\", \"class_label\", \"image_path\", \"label_path\"}\n",
    "    missing = expected_cols - set(df.columns)\n",
    "    assert not missing, f\"Missing columns in {path.name}: {sorted(missing)}\"\n",
    "\n",
    "    # Clean and coerce types\n",
    "    df = df.copy()\n",
    "    df[\"case_id\"] = df[\"case_id\"].astype(str).str.strip()\n",
    "    df[\"image_path\"] = df[\"image_path\"].astype(str).str.strip()\n",
    "    df[\"label_path\"] = df[\"label_path\"].astype(str).str.strip()\n",
    "    df[\"class_label\"] = pd.to_numeric(df[\"class_label\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "    # Optional: drop rows whose files don‚Äôt exist\n",
    "    if check_files:\n",
    "        def exists(p: str) -> bool: return Path(p).exists()\n",
    "        bad_mask = (~df[\"image_path\"].map(exists)) | (~df[\"label_path\"].map(exists))\n",
    "        if bad_mask.any():\n",
    "            n_bad = int(bad_mask.sum())\n",
    "            print(f\"[WARN] {n_bad} rows dropped from {path.name} due to missing files\")\n",
    "            df = df.loc[~bad_mask]\n",
    "\n",
    "    # Build MONAI-style dicts\n",
    "    items: List[Dict[str, Any]] = [\n",
    "        {\n",
    "            \"case_id\": r[\"case_id\"],\n",
    "            \"image\": r[\"image_path\"],\n",
    "            \"label\": r[\"label_path\"],\n",
    "            \"class_label\": int(r[\"class_label\"]),\n",
    "        }\n",
    "        for _, r in df.iterrows()\n",
    "    ]\n",
    "    return items\n",
    "\n",
    "train_items = read_unified_csv(TRAIN_CSV)\n",
    "val_items = read_unified_csv(VAL_CSV)\n",
    "test_items = read_unified_csv(TEST_CSV)\n",
    "\n",
    "len(train_items), len(val_items), len(test_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dc2c7-338e-43a7-9756-2f773f3c68f6",
   "metadata": {},
   "source": [
    "## üß™ Label Quality Control (QC) and Morphological Protection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed57125-07a4-43f0-84d2-efdf341085d3",
   "metadata": {},
   "source": [
    "This cell defines tools to assess and preserve label integrity across preprocessing or patch-based transformations, especially important for 3D medical segmentation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© `LabelQC` Class ‚Äì Shrinkage Detection\n",
    "\n",
    "Tracks how much segmentation labels shrink during processing (e.g., spatial cropping or transformations), which could harm training and evaluation.\n",
    "\n",
    "#### üîç Key Features:\n",
    "- `shrink_warn_threshold`: Warns if **label volume is reduced by more than this fraction** (default 35% loss).\n",
    "- `update(...)`: Compares voxel count before and after a transformation for a given `case_id`. If too much shrinkage is detected, it:\n",
    "  - Increments warning count.\n",
    "  - Saves the case info (`case_id`, voxel counts, shrinkage ratio).\n",
    "  - Prints a warning if `verbose=True`.\n",
    "\n",
    "#### üìä Usage:\n",
    "This class can be instantiated and used to:\n",
    "1. Track voxel shrinkage per case.\n",
    "2. Summarize how often labels were affected.\n",
    "3. Collect examples of problematic cases.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ `binary_dilate_then_erode(...)` ‚Äì Morphological Closing Operation\n",
    "\n",
    "Applies **binary dilation followed by erosion** to \"close\" small gaps in segmentation masks, helping preserve fragile or fragmented structures.\n",
    "\n",
    "### üîç Inside the `binary_dilate_then_erode` Function\n",
    "\n",
    "| Step                     | What's Happening                                              | Why It's Useful                                      |\n",
    "|--------------------------|---------------------------------------------------------------|------------------------------------------------------|\n",
    "| `mask > 0`               | Converts the mask into a binary format (1 = tumor, 0 = background) | Helps clearly define foreground (tumor) vs background |\n",
    "| `generate_binary_structure(...)` | Defines the neighborhood connectivity: 6, 18, or 26 neighbors | Controls how \"connected\" the dilation/erosion should be |\n",
    "| `binary_dilation(...)`  | Expands the foreground region slightly outward                | Fills small holes and connects broken regions        |\n",
    "| `binary_erosion(...)`   | Shrinks the region back to near-original size                 | Keeps filled gaps but removes over-expansion         |\n",
    "| `astype(np.uint8)`      | Converts the output to binary mask format (0s and 1s)         | Ensures compatibility with later processing steps    |\n",
    "\n",
    "#### üß¨ Parameters:\n",
    "- `mask`: 3D NumPy array representing a binary label mask.\n",
    "- `radius_vox`: Number of iterations for dilation/erosion. `0` means no operation.\n",
    "- `connectivity`: Determines voxel connectivity (1 = faces, 2 = faces + edges, 3 = faces + edges + corners).\n",
    "\n",
    "#### üßº Purpose:\n",
    "- Prevents label loss during cropping or patching.\n",
    "- Helps maintain thin structures (e.g., tumor edges) which may be disconnected in preprocessing.\n",
    "\n",
    "#### üîÅ Output:\n",
    "Returns a cleaned and connected binary mask (`np.uint8`) after closing.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why This Matters\n",
    "During patch-based segmentation or augmentations, small structures like tumors can be lost if not preserved. These tools:\n",
    "- Warn about significant loss of label volume.\n",
    "- Allow label smoothing while maintaining spatial integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80733c4c-07fc-4163-be3c-5522420385f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aab17ad-0be4-4ce3-8538-f6800b388bcc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a579666-642f-4161-9bdb-27f1dec62e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelQC:\n",
    "    \"\"\"\n",
    "    Tracks label shrinkage across patches/cases.\n",
    "    - shrink_warn_threshold: fraction of volume lost (e.g., 0.35 ‚Üí warn if after < 65% of before)\n",
    "    - verbose: if True, prints a line for each warning\n",
    "    \"\"\"\n",
    "    def __init__(self, shrink_warn_threshold: float = 0.35, verbose: bool = True):\n",
    "        self.shrink_warn_threshold = float(shrink_warn_threshold)\n",
    "        self.verbose = bool(verbose)\n",
    "        self.total: int = 0\n",
    "        self.warn: int = 0\n",
    "        self.flagged_examples: List[Tuple[str, int, int, float]] = []  # (case_id, before, after, ratio)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total = 0\n",
    "        self.warn = 0\n",
    "        self.flagged_examples.clear()\n",
    "\n",
    "    @property\n",
    "    def rate(self) -> float:\n",
    "        return self.warn / max(1, self.total)\n",
    "\n",
    "    def update(self, before_voxels: int, after_voxels: int, case_id: str) -> None:\n",
    "        self.total += 1\n",
    "        if before_voxels <= 0:\n",
    "            return\n",
    "        ratio = (after_voxels + 1e-6) / (before_voxels + 1e-6)\n",
    "        if ratio < (1.0 - self.shrink_warn_threshold):\n",
    "            self.warn += 1\n",
    "            self.flagged_examples.append((case_id, int(before_voxels), int(after_voxels), float(ratio)))\n",
    "            if self.verbose:\n",
    "                print(f\"[QC] label shrinkage: {case_id} before={before_voxels} after={after_voxels} ratio={ratio:.3f}\")\n",
    "\n",
    "    def summary(self) -> None:\n",
    "        print(f\"[QC] shrinkage warnings: {self.warn}/{self.total} (rate={self.rate:.4f})\")\n",
    "\n",
    "def binary_dilate_then_erode(mask: np.ndarray, radius_vox: int = 1, connectivity: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Light morphological close (dilate then erode) to protect thin/fragmented labels.\n",
    "    - mask: 3D array (D,H,W). Nonzeros are treated as foreground.\n",
    "    - radius_vox: number of iterations for dilation and erosion. 0 ‚Üí no-op.\n",
    "    - connectivity: 1 (faces), 2 (faces+edges), or 3 (faces+edges+corners).\n",
    "    Returns uint8 mask (0/1).\n",
    "    \"\"\"\n",
    "    if radius_vox <= 0:\n",
    "        return (mask > 0).astype(np.uint8)\n",
    "    if mask.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D mask, got shape {mask.shape}\")\n",
    "\n",
    "    mask_bool = (mask > 0)\n",
    "    structure = ndi.generate_binary_structure(rank=3, connectivity=int(connectivity))\n",
    "    dil = ndi.binary_dilation(mask_bool, structure=structure, iterations=int(radius_vox))\n",
    "    ero = ndi.binary_erosion(dil, structure=structure, iterations=int(radius_vox))\n",
    "    return ero.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606db4a7-e45d-4102-be96-e883bbc7a216",
   "metadata": {},
   "source": [
    "## üîÑ Data Transforms: Preprocessing, Augmentation, and Label Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f63fa9-2648-4e24-91d3-e6c37f064cca",
   "metadata": {},
   "source": [
    "This cell defines the full preprocessing and augmentation pipeline for **training and validation** using MONAI‚Äôs transform framework. It includes custom logic to ensure tumor labels are preserved even after spatial operations.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Custom Transform: `LabelPostProcessd`\n",
    "\n",
    "This transform is applied **after spacing and augmentation**, specifically to the **segmentation label mask**:\n",
    "\n",
    "#### üîç What it does:\n",
    "1. **Shape Matching**: If the label and image shapes differ, it resizes the label to match the image using **nearest-neighbor interpolation** (avoids smoothing or partial volume effects).\n",
    "2. **Light Morphology**: Applies `binary_dilate_then_erode()` to prevent thin or small tumors from vanishing due to interpolation, cropping, or resampling.\n",
    "3. **QC Tracking**: Adds two keys to the output dict:\n",
    "   - `qc_before_vox`: Number of voxels labeled as tumor *before* processing\n",
    "   - `qc_after_vox`: Number of voxels labeled as tumor *after* processing\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Transform Components Breakdown\n",
    "\n",
    "#### üîÑ `common_load`\n",
    "Shared transforms for both training and validation:\n",
    "- `LoadImaged`: Load NIfTI images and labels.\n",
    "- `EnsureChannelFirstd`: Converts image shape from `[D, H, W]` to `[1, D, H, W]`.\n",
    "- `EnsureTyped`: Ensures tensors are in PyTorch format.\n",
    "- `Orientationd`: Aligns all images to RAS orientation.\n",
    "- `Spacingd`: Resamples images and labels to common voxel spacing:\n",
    "  - **Image**: uses bilinear interpolation.\n",
    "  - **Label**: uses nearest-neighbor to avoid label corruption.\n",
    "\n",
    "#### üß† `intensity_train`\n",
    "Training-only transforms:\n",
    "- `ScaleIntensityRanged`: Clips and scales intensity values to `[0, 1]` range (from raw range 0‚Äì3000 HU).\n",
    "- `RandFlipd`: Random flip along each axis (20% chance).\n",
    "- `RandRotate90d`: Random 90¬∞ rotation (20% chance).\n",
    "- `RandAffined`: Random affine transformation (rotation, scaling).\n",
    "- `SpatialPadd`: Pads to ensure minimum patch size before cropping.\n",
    "- `RandCropByPosNegLabeld`: Extracts patches with a mix of positive (tumor) and negative (background) regions.\n",
    "\n",
    "#### üìè `intensity_val`\n",
    "Validation-only transforms:\n",
    "- Only includes `ScaleIntensityRanged` to standardize intensities, without applying any random augmentations.\n",
    "\n",
    "---\n",
    "\n",
    "### üè∑Ô∏è `CastClassLabeld` ‚Äì Classification Label Casting\n",
    "\n",
    "A custom transform that ensures the `\"class_label\"` is:\n",
    "- Converted to a PyTorch `float32` tensor.\n",
    "- This is required for compatibility with loss functions used for classification.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Compose Final Transform Pipelines\n",
    "\n",
    "- `train_transforms`: Combines loading, augmentation, label postprocessing, and classification label casting.\n",
    "- `val_transforms`: Similar to training, but without random augmentations.\n",
    "\n",
    "Each sample (image, label, class_label) is processed through these transformations before being fed into the model.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This setup ensures that:\n",
    "- Input images are normalized and aligned in space.\n",
    "- Labels are protected from degradation during spatial transforms.\n",
    "- Both segmentation and classification targets are properly prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6efcbf-016f-4c2f-93eb-9c2743d78d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms: spacing standardization and intensity scale\n",
    "# Labels: post-process after Spacingd to preserve small lesions (no extra soft resampling)\n",
    "\n",
    "from monai.transforms import MapTransform\n",
    "\n",
    "class LabelPostProcessd(MapTransform):\n",
    "    \"\"\"\n",
    "    - Ensures label shape matches image shape (nearest-neighbor up/down-sample if needed)\n",
    "    - Optional light morphology (dilate‚Üíerode) to protect thin/speck lesions\n",
    "    - Records QC counts: qc_before_vox, qc_after_vox\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, ref_key: str = \"image\", morph_radius: int = 1, allow_missing_keys: bool = False):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.ref_key = ref_key\n",
    "        self.morph_radius = int(morph_radius)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        if \"label\" not in d:\n",
    "            return d\n",
    "\n",
    "        label = d[\"label\"]  # Tensor [1, D, H, W]\n",
    "        img = d.get(self.ref_key, None)\n",
    "        before_vox = int((label > 0).sum().item())\n",
    "\n",
    "        # Match label to image grid if needed (nearest to avoid smoothing)\n",
    "        if img is not None and label.shape[1:] != img.shape[1:]:\n",
    "            label = torch.nn.functional.interpolate(\n",
    "                label.float(), size=img.shape[1:], mode=\"nearest\"\n",
    "            ).long()\n",
    "\n",
    "        # Optional light morphology\n",
    "        if self.morph_radius > 0:\n",
    "            arr = (label > 0).cpu().numpy().astype(np.uint8)  # [1, D, H, W]\n",
    "            arr = binary_dilate_then_erode(arr[0], radius_vox=self.morph_radius)[None]  # back to [1, ...]\n",
    "            label = torch.as_tensor(arr, dtype=torch.long, device=d[\"label\"].device)\n",
    "\n",
    "        after_vox = int((label > 0).sum().item())\n",
    "\n",
    "        d[\"label\"] = label\n",
    "        d[\"qc_before_vox\"] = before_vox\n",
    "        d[\"qc_after_vox\"] = after_vox\n",
    "        return d\n",
    "\n",
    "\n",
    "# Common I/O and spacing (labels via nearest to avoid smoothing)\n",
    "common_load = [\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "    Spacingd(keys=[\"image\", \"label\"], pixdim=TARGET_SPACING, mode=(\"bilinear\", \"nearest\")),\n",
    "]\n",
    "\n",
    "from monai.transforms import RandCropByPosNegLabeld, SpatialPadd\n",
    "\n",
    "# Training intensity + spatial augs\n",
    "intensity_train = [\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=3000, b_min=0.0, b_max=1.0, clip=True),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=[0, 1, 2], prob=0.2),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.2, max_k=3),\n",
    "    RandAffined(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        rotate_range=(math.pi/36, math.pi/36, math.pi/36),\n",
    "        scale_range=(0.1, 0.1, 0.1),\n",
    "        mode=(\"bilinear\", \"nearest\"),\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    SpatialPadd(keys=[\"image\", \"label\"], spatial_size=PATCH_SIZE),\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=PATCH_SIZE,\n",
    "        pos=1, neg=1, num_samples=1, image_key=\"image\",\n",
    "        allow_smaller=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Validation intensity only\n",
    "intensity_val = [\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0, a_max=3000, b_min=0.0, b_max=1.0, clip=True),\n",
    "]\n",
    "\n",
    "# Cast classification label\n",
    "class CastClassLabeld(MapTransform):\n",
    "    def __init__(self, keys, allow_missing_keys=False):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        if \"class_label\" in d:\n",
    "            d[\"class_label\"] = torch.as_tensor(d[\"class_label\"], dtype=torch.float32)\n",
    "        return d\n",
    "\n",
    "# Assemble transforms\n",
    "# - Post label process uses nearest resample if shapes differ (no soft one-hot resample)\n",
    "# - Morph radius=1; raise to 2 if QC indicates too many tiny lesions vanish\n",
    "post_label_preserve = [LabelPostProcessd(keys=[\"label\"], ref_key=\"image\", morph_radius=1)]\n",
    "\n",
    "train_transforms = Compose(common_load + intensity_train + post_label_preserve + [CastClassLabeld(keys=[\"class_label\"])])\n",
    "val_transforms = Compose(common_load + intensity_val + post_label_preserve + [CastClassLabeld(keys=[\"class_label\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abc22f-20a7-451f-bd89-7595f3fa257c",
   "metadata": {},
   "source": [
    "## üì¶ Dataset and DataLoader Setup with Quality Control and Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2e893-5cb1-4063-9f44-5206f249b508",
   "metadata": {},
   "source": [
    "This cell prepares the datasets and data loaders for both training and validation. It also initializes the label shrinkage QC trackers and ensures reproducible data loading.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Quality Control Trackers\n",
    "- `qc_train` and `qc_val` are instances of `LabelQC` (defined earlier).\n",
    "- They track how often tumor labels shrink significantly after preprocessing (e.g., due to cropping or augmentation).\n",
    "- `shrink_warn_threshold=0.35`: Warn if more than 35% of the tumor voxels are lost.\n",
    "- `verbose=True`: Logs detailed info for each case that triggers a warning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Reproducible DataLoader Workers\n",
    "To ensure consistent behavior across runs (especially in a multi-worker setup), we define:\n",
    "\n",
    "#### üîß `seed_worker(worker_id)`\n",
    "- Sets consistent seeds for NumPy, Python's `random`, and PyTorch using a global `SEED`.\n",
    "- Ensures the same augmentations, crops, and shuffles are applied for the same input data when re-running experiments.\n",
    "\n",
    "#### üé≤ Random Generator\n",
    "- `torch.Generator()` with `.manual_seed(SEED)` is used to further ensure deterministic shuffling and batch sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Dataset Setup with `CacheDataset`\n",
    "- **`CacheDataset`**: A MONAI dataset that caches transformed data in memory to reduce preprocessing overhead.\n",
    "- `cache_rate=0.0`: Disables caching to reduce RAM usage (safe for large datasets or limited memory). Set to `>0.0` to enable partial caching later if needed.\n",
    "- `train_ds` and `val_ds` use the respective transforms (`train_transforms`, `val_transforms`) defined earlier.\n",
    "\n",
    "---\n",
    "\n",
    "### üöö DataLoaders\n",
    "Used to feed data into the model during training and validation.\n",
    "\n",
    "#### üîß Parameters:\n",
    "- `batch_size=1`: Each batch contains one 3D volume (common for volumetric segmentation due to GPU memory limits).\n",
    "- `shuffle=True` for training (randomized batches), `False` for validation.\n",
    "- `num_workers`: Number of subprocesses used for data loading. Higher = faster loading (if memory allows).\n",
    "- `pin_memory=True`: Speeds up host-to-GPU data transfer when using CUDA.\n",
    "- `persistent_workers=True`: Keeps DataLoader workers alive between epochs for efficiency.\n",
    "- `worker_init_fn=seed_worker`: Ensures reproducible behavior in worker threads.\n",
    "- `prefetch_factor=2`: Controls how many batches to preload per worker (improves throughput).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "This setup ensures:\n",
    "- Labels are monitored for quality.\n",
    "- Data loading is fast and reproducible.\n",
    "- Datasets are transformed correctly and efficiently passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48364ab7-812e-4c57-af75-cad1f36b53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC accumulators (patch-level stats already added in transforms and training loop)\n",
    "qc_train = LabelQC(shrink_warn_threshold=0.35, verbose=True)\n",
    "qc_val = LabelQC(shrink_warn_threshold=0.35, verbose=True)\n",
    "\n",
    "# Reproducible dataloader workers\n",
    "def seed_worker(worker_id: int):\n",
    "    base_seed = SEED\n",
    "    np.random.seed(base_seed + worker_id)\n",
    "    random.seed(base_seed + worker_id)\n",
    "    torch.manual_seed(base_seed + worker_id)\n",
    "\n",
    "use_pin = (device.type == \"cuda\")\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Datasets (cache_rate=0.0 to avoid RAM pressure; switch to >0.0 if you want caching)\n",
    "train_ds = CacheDataset(data=train_items, transform=train_transforms, cache_rate=0.0, num_workers=0)\n",
    "val_ds   = CacheDataset(data=val_items,   transform=val_transforms,   cache_rate=0.0, num_workers=0)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=use_pin,\n",
    "    persistent_workers=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=use_pin,\n",
    "    persistent_workers=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b3373-5177-46eb-8bd5-d1967c46cb5a",
   "metadata": {},
   "source": [
    "## üß† Model Architecture: 3D Segmentation + Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85716878-2db5-4929-9db5-3e79d698a5f8",
   "metadata": {},
   "source": [
    "This cell defines the **core model architecture**, combining a 3D segmentation network with a classification head, and sets up losses, optimizer, and evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± 1. Segmentation Network: `DynUNet` (nnU-Net-like)\n",
    "\n",
    "We use `DynUNet` from MONAI ‚Äî a highly flexible and modular 3D U-Net backbone inspired by nnU-Net.\n",
    "\n",
    "#### ‚öôÔ∏è Configuration:\n",
    "- `spatial_dims=3`: For 3D volumetric data (e.g. brain MRIs).\n",
    "- `in_channels=1`: Input is a single-channel MRI image.\n",
    "- `out_channels=2`: Output is a binary segmentation mask (background vs tumor).\n",
    "- `kernel_size`: Defines convolution kernel sizes at each stage (6 total).\n",
    "- `strides`: Downsampling steps (progressively reduce spatial resolution).\n",
    "- `upsample_kernel_size`: Used in the decoder for upsampling.\n",
    "- `norm_name=\"instance\"`: Applies instance normalization.\n",
    "- `deep_supervision=False`: Only the final output is used for loss calculation.\n",
    "\n",
    "The model is transferred to the active `device` (GPU or CPU).\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ 2. Classification Head: `LazyClassificationHead`\n",
    "\n",
    "A lightweight classification module that predicts **Glioma vs Mets** using features extracted from the encoder‚Äôs bottleneck.\n",
    "\n",
    "#### üîç Design:\n",
    "- `AdaptiveAvgPool3d(1)`: Reduces the 3D feature map to a 1√ó1√ó1 spatial dimension (global pooling).\n",
    "- `fc`: A fully connected layer, initialized lazily on first forward pass to match input size.\n",
    "- `num_classes=1`: Binary classification output (logits for Mets probability).\n",
    "\n",
    "---\n",
    "\n",
    "### ü™ù 3. Feature Hook for Classification\n",
    "\n",
    "To allow classification without interfering with segmentation, we use a **forward hook** to tap into the encoder‚Äôs bottleneck output:\n",
    "- The hook captures intermediate features (`encoder_feat[\"x\"]`) for use in the classification head.\n",
    "- Attached to either `seg_net.bottleneck` or fallback `encoder4`, depending on DynUNet structure.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è 4. Loss Functions\n",
    "\n",
    "- `seg_loss_fn`: `DiceCELoss` combines Dice loss (for overlap) and CrossEntropy (for pixel-wise classification).\n",
    "  - `to_onehot_y=True`, `softmax=True`: Converts target to one-hot and applies softmax to logits.\n",
    "- `cls_loss_fn`: `BCEWithLogitsLoss` for binary classification from raw logits (Glioma vs Mets).\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ 5. Optimizer and AMP\n",
    "\n",
    "- `AdamW`: Optimizer for both segmentation and classification parameters.\n",
    "  - `lr=1e-5`, `weight_decay=1e-5`: Stable learning rate and regularization.\n",
    "- `GradScaler`: Enables **Automatic Mixed Precision (AMP)** to reduce memory usage and speed up training on GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä 6. Evaluation Metric\n",
    "\n",
    "- `dice_metric`: MONAI‚Äôs Dice metric (averaged across non-background classes).\n",
    "  - `include_background=False`: Only evaluates tumor class (not background).\n",
    "  - `reduction=\"mean\"`: Computes mean Dice score across batch or volume.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This cell finalizes the dual-task learning setup:\n",
    "- **Segmentation**: Tumor mask prediction from MRI.\n",
    "- **Classification**: Glioma vs Mets nature of the tumor.\n",
    "- Optimized jointly using shared encoder features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3b37d-fde5-42ba-959a-b6ac8323462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation network (nnU-Net-like DynUNet)\n",
    "seg_net = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    kernel_size=[3, 3, 3, 3, 3, 3],     # 6 stages\n",
    "    strides=[1, 2, 2, 2, 2, 2],         # length matches kernel_size\n",
    "    upsample_kernel_size=[2, 2, 2, 2, 2],\n",
    "    norm_name=\"instance\",\n",
    "    deep_supervision=False,\n",
    ").to(device)\n",
    "\n",
    "# Classification head (lazy: initializes fully-connected layer on first forward)\n",
    "class LazyClassificationHead(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, feat: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(feat).flatten(1)\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(x.shape[1], self.num_classes).to(x.device)\n",
    "        return self.fc(x)\n",
    "\n",
    "cls_head = LazyClassificationHead(num_classes=1).to(device)\n",
    "\n",
    "# Hook to capture encoder bottleneck features for classification\n",
    "encoder_feat = {\"x\": None}\n",
    "def hook_fn(module, input, output):\n",
    "    encoder_feat[\"x\"] = output\n",
    "\n",
    "# Attach hook to a stable location in DynUNet\n",
    "if hasattr(seg_net, \"bottleneck\"):\n",
    "    seg_net.bottleneck.register_forward_hook(hook_fn)\n",
    "elif hasattr(seg_net, \"encoder4\"):\n",
    "    seg_net.encoder4.register_forward_hook(hook_fn)\n",
    "else:\n",
    "    print(\"[WARN] Could not attach hook; classification head may not receive features\")\n",
    "\n",
    "# Losses and optimizer\n",
    "seg_loss_fn = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "cls_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "params = list(seg_net.parameters()) + list(cls_head.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "# AMP scaler\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Segmentation metrics\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e84eb-d9ce-4467-9182-f9eedff27c14",
   "metadata": {},
   "source": [
    "## üìê Tensor Padding and Cropping Utilities for 3D Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f4508-5392-4d60-9a47-498e4c73246a",
   "metadata": {},
   "source": [
    "This cell defines two essential utility functions to **ensure compatibility of input volumes** with the model architecture ‚Äî especially for operations like convolutions, which often require spatial dimensions to be multiples of a specific factor (e.g., 16 or 32).\n",
    "\n",
    "---\n",
    "\n",
    "### üî≤ `pad_to_factor(...)`\n",
    "\n",
    "Pads a 5D tensor of shape `(B, C, D, H, W)` so that its **depth (D), height (H), and width (W)** are divisible by a given `factor`.\n",
    "\n",
    "#### ‚öôÔ∏è Why is this needed?\n",
    "Many segmentation models (like U-Net, DynUNet) perform multiple downsampling operations (via strides or pooling). If the input dimensions aren‚Äôt divisible by the downsampling factor, shape mismatches may occur during upsampling in the decoder.\n",
    "\n",
    "#### üß† How it works:\n",
    "- Calculates the **next multiple** of `factor` for each spatial dimension.\n",
    "- Adds padding only to the **right, bottom, and back** sides ‚Äî so the origin stays unchanged (important for spatial alignment with labels).\n",
    "- Supports padding with any value (`value=0.0` by default) and padding mode (e.g., `\"constant\"`).\n",
    "\n",
    "#### üîÅ Parameters:\n",
    "- `x`: Input tensor of shape `(B, C, D, H, W)`.\n",
    "- `factor`: Scalar (e.g., 32) or tuple `(fD, fH, fW)`.\n",
    "- `return_pad`: If `True`, returns both the padded tensor and the padding applied.\n",
    "\n",
    "#### üßæ Example:\n",
    "If an input has shape `[1, 1, 123, 256, 249]` and factor is `32`, it will be padded to `[1, 1, 128, 256, 256]`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÇÔ∏è `crop_to_shape(...)`\n",
    "\n",
    "Crops a padded 5D tensor `(B, C, D, H, W)` **back down** to a target shape `(D, H, W)`, used typically **after inference** to remove padding and recover the original dimensions.\n",
    "\n",
    "#### üîß How it works:\n",
    "- Performs a simple slice from the beginning (`:Dz, :Hy, :Wx`) along each spatial axis.\n",
    "- Ensures that output spatial dimensions match exactly with the ground truth.\n",
    "\n",
    "#### üßæ Example:\n",
    "If output after segmentation is `[1, 1, 128, 256, 256]` and original shape was `[123, 240, 240]`, this function will crop it back accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "These two functions work together to:\n",
    "- üß± **Prepare inputs** for model inference or training (via `pad_to_factor`)\n",
    "- ‚úÇÔ∏è **Restore outputs** back to the original shape (via `crop_to_shape`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701d691-731f-4b5f-91ae-9500be5dc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_factor(x: torch.Tensor, factor=32, return_pad: bool = False, mode: str = \"constant\", value: float = 0.0):\n",
    "    \"\"\"\n",
    "    Pads a 5D tensor (B, C, D, H, W) so each spatial dim is a multiple of `factor`.\n",
    "    - factor: int or (fD, fH, fW)\n",
    "    - Pads only on the \"right/bottom/back\" to avoid shifting coordinates\n",
    "    - If return_pad=True, also returns the pad tuple (Wl, Wr, Hl, Hr, Dl, Dr)\n",
    "    \"\"\"\n",
    "    assert x.dim() == 5, f\"Expected 5D tensor (B,C,D,H,W), got shape {tuple(x.shape)}\"\n",
    "    if isinstance(factor, int):\n",
    "        fD = fH = fW = factor\n",
    "    else:\n",
    "        assert len(factor) == 3, \"factor must be int or 3-tuple\"\n",
    "        fD, fH, fW = factor\n",
    "\n",
    "    B, C, D, H, W = x.shape\n",
    "    def next_m(s, f): return ((s + f - 1) // f) * f\n",
    "    Dn, Hn, Wn = next_m(D, fD), next_m(H, fH), next_m(W, fW)\n",
    "    pd, ph, pw = Dn - D, Hn - H, Wn - W\n",
    "    pad = (0, pw, 0, ph, 0, pd)  # (W_left, W_right, H_left, H_right, D_left, D_right)\n",
    "\n",
    "    if any(p > 0 for p in pad):\n",
    "        x = F.pad(x, pad, mode=mode, value=value)\n",
    "\n",
    "    return (x, pad) if return_pad else x\n",
    "\n",
    "def crop_to_shape(x: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crops tensor x (B,C,D,H,W) to spatial shape `shape` = (D,H,W), slicing from the start along each dim.\n",
    "    \"\"\"\n",
    "    assert x.dim() == 5 and len(shape) == 3, \"x must be 5D and shape must be (D,H,W)\"\n",
    "    Dz, Hy, Wx = map(int, shape)\n",
    "    return x[..., :Dz, :Hy, :Wx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc41140-f36c-4733-b180-d08ae9414a73",
   "metadata": {},
   "source": [
    "## üîç Sliding Window Inference Setup + Inference Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789749f6-9228-40c5-896a-92d11d86f1a0",
   "metadata": {},
   "source": [
    "This cell sets up the **inference engine** for 3D volumetric segmentation using a patch-based strategy and defines helper functions to:\n",
    "1. Move data to the correct device.\n",
    "2. Apply inference on padded volumes with automatic cropping to restore original shape.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† `SlidingWindowInferer`\n",
    "\n",
    "The `SlidingWindowInferer` allows running inference on large 3D MRI volumes by dividing them into smaller **overlapping patches**, running the model on each, and combining the outputs.\n",
    "\n",
    "#### üß∞ Parameters:\n",
    "- `roi_size=PATCH_SIZE`: Size of the 3D patch used during inference.\n",
    "- `sw_batch_size=1`: Number of patches to process in a mini-batch.\n",
    "- `overlap=PATCH_OVERLAP`: Amount of overlap between adjacent patches (e.g., 0.5 = 50% overlap). This improves prediction consistency across patch borders.\n",
    "- `mode=\"gaussian\"`: Overlapping areas are blended using a Gaussian weighting (gives smooth transitions).\n",
    "\n",
    "This is especially useful when the full 3D volume does not fit in GPU memory.\n",
    "\n",
    "---\n",
    "\n",
    "### üîå `to_device(batch, device)`\n",
    "\n",
    "Utility function that:\n",
    "- Transfers all `torch.Tensor` values in a dictionary (`batch`) to the specified `device` (e.g., GPU).\n",
    "- Non-tensor values (e.g., strings, integers) are passed through unchanged.\n",
    "\n",
    "Used during validation/inference when batches are loaded by the DataLoader on CPU but need to be moved to GPU.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ `sliding_infer_padded(...)`\n",
    "\n",
    "A wrapper for inference that:\n",
    "1. **Pads** the input volume so it is compatible with the model (dims divisible by `factor` like 32).\n",
    "2. Runs **sliding window inference** using the `inferer` object.\n",
    "3. **Crops** the output back to original shape (if `target_shape` is provided).\n",
    "\n",
    "#### üîÅ Parameters:\n",
    "- `images`: Input tensor of shape `(B, C, D, H, W)`.\n",
    "- `network`: The segmentation model (e.g., DynUNet).\n",
    "- `factor`: The factor to pad to (default: 32).\n",
    "- `target_shape`: Optional. If set, crops back to the original spatial size.\n",
    "\n",
    "This utility ensures robust and memory-efficient inference on full volumes, even if their sizes are irregular.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "These tools ensure:\n",
    "- Memory-efficient inference on large 3D volumes.\n",
    "- Compatibility with model requirements via dynamic padding.\n",
    "- Accurate spatial alignment by restoring original image dimensions after inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f099da9-afef-45b9-b733-ab4c40a076fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferer = SlidingWindowInferer(\n",
    "    roi_size=PATCH_SIZE,\n",
    "    sw_batch_size=1,\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    mode=\"gaussian\",\n",
    ")\n",
    "\n",
    "# Utils\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "def to_device(batch: Dict[str, Any], device: torch.device) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            out[k] = v.to(device, non_blocking=True)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def sliding_infer_padded(\n",
    "    images: torch.Tensor,\n",
    "    network: nn.Module,\n",
    "    factor: int = 32,\n",
    "    target_shape: Optional[Tuple[int, int, int]] = None,\n",
    ") -> torch.Tensor:\n",
    "    images_p, _ = pad_to_factor(images, factor=factor, return_pad=True)\n",
    "    logits = inferer(inputs=images_p, network=network)\n",
    "    if target_shape is not None and logits.shape[-3:] != tuple(target_shape):\n",
    "        logits = crop_to_shape(logits, target_shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944918a-0134-4f7e-8b62-44729389dab3",
   "metadata": {},
   "source": [
    "## üßæ Save Configuration Metadata for Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e984d5-fa1c-48b9-abc4-6def60d7c887",
   "metadata": {},
   "source": [
    "This cell captures all important settings and hyperparameters used in the current run and writes them to a JSON file (`config.json`) for **reproducibility, auditing, and sharing**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç `file_sha1(...)`: File Fingerprint Utility\n",
    "A helper function that:\n",
    "- Computes a **SHA-1 hash** of the file contents at the given path.\n",
    "- Returns `None` if the file cannot be read.\n",
    "- Used to **verify integrity and uniqueness** of the CSV files used for train/val/test splits.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† `config`: Experiment Metadata Dictionary\n",
    "\n",
    "This dictionary stores **all key configuration parameters** for this training run:\n",
    "\n",
    "#### üîë Run Info\n",
    "- `run_id`: Unique timestamped ID.\n",
    "- `seed`: Random seed used for reproducibility.\n",
    "- `device`: CPU or GPU.\n",
    "- `gpu_name`: Name of the active CUDA GPU (if available).\n",
    "\n",
    "#### üß≠ Preprocessing\n",
    "- `target_spacing`: Desired voxel spacing after resampling.\n",
    "- `patch_size`: Patch dimensions used during training and inference.\n",
    "- `inferer`: Settings for the sliding window inferer (ROI size, overlap, blending mode).\n",
    "\n",
    "#### üèóÔ∏è Model Architecture\n",
    "- Model type (`DynUNet`) and its core parameters (input/output channels, normalization, deep supervision).\n",
    "\n",
    "#### ‚öñÔ∏è Loss Functions\n",
    "- `seg`: Segmentation loss (`DiceCELoss`)\n",
    "- `cls`: Classification loss (`BCEWithLogitsLoss`)\n",
    "- `cls_weight`: Implied use for weighted loss combination (if used later).\n",
    "\n",
    "#### üöÄ Optimizer\n",
    "- Type (`AdamW`), learning rate, and weight decay.\n",
    "\n",
    "#### üîÅ Augmentations\n",
    "- Flip and rotation probabilities.\n",
    "- Affine transformation settings (rotation in degrees, scaling in %).\n",
    "- Cropping strategy.\n",
    "\n",
    "#### üß™ Transforms Notes\n",
    "- Describes how images and labels were processed:\n",
    "  - Image spacing via bilinear interpolation.\n",
    "  - Label spacing via nearest neighbor (to avoid smoothing).\n",
    "  - LabelPostProcessd with morphological protection (`morph_radius=1`).\n",
    "\n",
    "#### üóÇÔ∏è Dataset Splits\n",
    "- Paths to training/validation/testing CSVs.\n",
    "- SHA-1 hashes of each CSV to ensure data integrity and version control.\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Save to File\n",
    "The dictionary is saved as formatted JSON (`config.json`) in the current run directory. This file:\n",
    "- Serves as a **record of hyperparameters and pipeline settings**.\n",
    "- Can be used to **reproduce the exact experiment later**.\n",
    "- Can be shared or logged in experiment tracking systems.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "Capturing and saving configuration metadata like this ensures that all future reviewers (or our future review) can understand and recreate the model, training conditions, and dataset splits exactly as they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8ded9-fb10-431f-b8ea-749fd3a12583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_sha1(path: Path | str) -> str | None:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return hashlib.sha1(f.read()).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "config = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(device),\n",
    "    \"gpu_name\": (torch.cuda.get_device_name(0) if torch.cuda.is_available() else None),\n",
    "    \"target_spacing\": tuple(TARGET_SPACING),\n",
    "    \"patch_size\": tuple(PATCH_SIZE),\n",
    "    \"inferer\": {\"roi_size\": tuple(PATCH_SIZE), \"overlap\": float(PATCH_OVERLAP), \"mode\": \"gaussian\"},\n",
    "    \"model\": {\n",
    "        \"arch\": \"DynUNet\",\n",
    "        \"in_channels\": 1,\n",
    "        \"out_channels\": 2,\n",
    "        \"deep_supervision\": False,\n",
    "        \"norm_name\": \"instance\",\n",
    "    },\n",
    "    \"loss\": {\"seg\": \"DiceCELoss\", \"cls\": \"BCEWithLogitsLoss\", \"cls_weight\": 0.3},\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "        \"weight_decay\": float(optimizer.param_groups[0].get(\"weight_decay\", 0.0)),\n",
    "    },\n",
    "    \"augs\": {\n",
    "        \"flip_p\": 0.2, \"rot90_p\": 0.2,\n",
    "        \"affine\": {\"rot_deg\": 5, \"scale_pct\": 10},\n",
    "        \"crop\": \"RandCropByPosNegLabel\",\n",
    "    },\n",
    "    # Reflect current pipeline (nearest for labels + light morph).\n",
    "    \"transforms_notes\": \"Spacingd (image=bilinear, label=nearest), LabelPostProcessd(morph_radius=1)\",\n",
    "    \"splits\": {\n",
    "        \"train_csv\": str(TRAIN_CSV), \"val_csv\": str(VAL_CSV), \"test_csv\": str(TEST_CSV),\n",
    "        \"train_csv_sha1\": file_sha1(TRAIN_CSV), \"val_csv_sha1\": file_sha1(VAL_CSV), \"test_csv_sha1\": file_sha1(TEST_CSV),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(CONFIG_JSON, \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"Saved config:\", CONFIG_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f0504-21be-47eb-b663-197dec99b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment snapshot (for full reproducibility)\n",
    "env = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"os\": platform.platform(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"torch_cuda\": (torch.version.cuda if torch.cuda.is_available() else None),\n",
    "    \"cudnn\": (torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None),\n",
    "    \"monai\": __import__(\"monai\").__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"gpu_count\": (torch.cuda.device_count() if torch.cuda.is_available() else 0),\n",
    "    \"gpu_name\": (torch.cuda.get_device_name(0) if torch.cuda.is_available() else None),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "# List all GPU names if multiple are present\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    env[\"gpu_names\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "\n",
    "# Optional: exact package set (can be large)\n",
    "try:\n",
    "    env[\"pip_freeze\"] = subprocess.check_output([\"pip\", \"freeze\"]).decode().splitlines()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "with open(ENV_JSON, \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "print(\"Saved env:\", ENV_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18fcd2-06f7-4e25-8a18-e7b8dd3f0fd7",
   "metadata": {},
   "source": [
    "## üîÅ Training & Validation Loop with Resume Logic, Metrics, and Checkpointing\n",
    "\n",
    "This block implements the **core training and validation loop** for the joint segmentation + classification model. It includes logic for:\n",
    "- Resuming from checkpoints\n",
    "- Training over multiple epochs\n",
    "- Tracking segmentation and classification metrics\n",
    "- Saving model states based on validation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a434b3c-bd3b-4219-8e40-273d8b910b2f",
   "metadata": {},
   "source": [
    "### üíæ 1. Resume from Last or Best Checkpoint\n",
    "- `find_resume_ckpt()`: Looks for `last.pt` or `best.pt` in the current run directory (`RUN_DIR`) or fallback directory (`ckpt_dir`).\n",
    "- Loads model weights, optimizer state, scaler (for AMP), and tracking variables like `start_epoch`, `best_val_dice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f98b5-11da-4cba-ab68-1f6fc4b9b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_resume_ckpt() -> Path:\n",
    "    candidates = []\n",
    "    # Prefer run-scoped last/best if present\n",
    "    if \"RUN_DIR\" in globals():\n",
    "        candidates += [RUN_DIR / \"last.pt\", RUN_DIR / \"best.pt\"]\n",
    "    # Fallback to global dir\n",
    "    candidates += [ckpt_dir / \"last.pt\", ckpt_dir / \"best.pt\"]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No checkpoint found in {ckpt_dir} (looked for last.pt/best.pt in run/global dirs).\")\n",
    "\n",
    "RESUME_CKPT = find_resume_ckpt()\n",
    "print(f\"Resuming from: {RESUME_CKPT}\")\n",
    "\n",
    "# Initialize lazy cls_head.fc before loading (captures encoder channels safely)\n",
    "with torch.no_grad():\n",
    "    was_training = seg_net.training\n",
    "    seg_net.eval()\n",
    "    encoder_feat[\"x\"] = None\n",
    "    dummy = torch.zeros(1, 1, 64, 64, 64, device=device)\n",
    "    _ = seg_net(dummy)\n",
    "    feat = encoder_feat[\"x\"] if encoder_feat[\"x\"] is not None else dummy\n",
    "    _ = cls_head(feat)  # initializes cls_head.fc lazily\n",
    "    seg_net.train(was_training)\n",
    "\n",
    "ckpt = torch.load(RESUME_CKPT, map_location=device)\n",
    "\n",
    "missing, unexpected = seg_net.load_state_dict(ckpt[\"seg\"], strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] seg_net state_dict mismatches. missing={missing}, unexpected={unexpected}\")\n",
    "missing, unexpected = cls_head.load_state_dict(ckpt[\"cls\"], strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] cls_head state_dict mismatches. missing={missing}, unexpected={unexpected}\")\n",
    "\n",
    "if \"optimizer\" in ckpt:\n",
    "    try:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] could not load optimizer state: {e}\")\n",
    "if \"scaler\" in ckpt:\n",
    "    try:\n",
    "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] could not load scaler state: {e}\")\n",
    "\n",
    "# Epoch planning\n",
    "start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "best_val_dice = float(ckpt.get(\"best_val_dice\", -1.0))\n",
    "EPOCHS_NEXT = 50\n",
    "end_epoch = start_epoch + EPOCHS_NEXT - 1\n",
    "\n",
    "print(f\"Resuming at epoch {start_epoch} (best_val_dice={best_val_dice:.4f}); training until epoch {end_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1728d-f173-4dd8-84da-db2fc1f1fccd",
   "metadata": {},
   "source": [
    "### Open metrics CSV (segmentation + training stats)\n",
    "\n",
    "### üìä 2. Metric Setup\n",
    "- `metrics_f`, `metrics_w`: File handle and writer for logging per-epoch metrics to CSV.\n",
    "- `HausdorffDistanceMetric`: For spatial accuracy of segmentation (`hd95`).\n",
    "- `new_cm()` and `update_cm()`: Confusion matrix tracker (TP, FP, FN, TN) used for classification metrics: Accuracy, Recall, F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2452e-adff-44bb-8605-9d4ef3f62ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open metrics CSV (segmentation + training stats)\n",
    "# Close with: metrics_f.close() at the end of training\n",
    "metrics_f, metrics_w = write_csv_header(\n",
    "    METRICS_CSV,\n",
    "    [\"epoch\",\"split\",\"loss\",\"dice\",\"acc\",\"recall\",\"f1\",\"hd95\",\"lr\",\"seconds\",\"qc_warns\",\"qc_total\",\"qc_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e41f33-9141-4a24-9675-94374fd854db",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "hd95_tr = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile=95)\n",
    "hd95_val = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile=95)\n",
    "\n",
    "def new_cm():\n",
    "    return {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "\n",
    "def update_cm(cm, ypred, ytrue):\n",
    "    yp = (ypred > 0).to(torch.bool)\n",
    "    yt = (ytrue > 0).to(torch.bool)\n",
    "    cm[\"tp\"] += (yp & yt).sum().item()\n",
    "    cm[\"fp\"] += (yp & ~yt).sum().item()\n",
    "    cm[\"fn\"] += (~yp & yt).sum().item()\n",
    "    cm[\"tn\"] += (~yp & ~yt).sum().item()\n",
    "\n",
    "def cm_metrics(cm):\n",
    "    tp, fp, fn, tn = cm[\"tp\"], cm[\"fp\"], cm[\"fn\"], cm[\"tn\"]\n",
    "    acc = (tp + tn) / max(1, tp + fp + fn + tn)\n",
    "    rec = tp / max(1, tp + fn)\n",
    "    pre = tp / max(1, tp + fp)\n",
    "    f1 = (2 * pre * rec) / max(eps, pre + rec)\n",
    "    return acc, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aaa60d-1844-48c4-893d-9a941151574a",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49de9d1-b8f8-4f74-b1bc-279e458fbe3f",
   "metadata": {},
   "source": [
    "#### üîÑ Lazy Initialization of `cls_head.fc`\n",
    "Since the classification head is lazily defined based on encoder output shape:\n",
    "- A dummy input is passed through `seg_net`, and its bottleneck output is used to initialize `cls_head`.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ 3. Training Loop\n",
    "\n",
    "For each epoch from `start_epoch` to `end_epoch`:\n",
    "\n",
    "#### üîß Training Phase\n",
    "- Sets both `seg_net` and `cls_head` to `train()` mode.\n",
    "- Iterates over the training DataLoader:\n",
    "  - Moves data to the correct device.\n",
    "  - Updates label shrinkage quality control (`qc_train`).\n",
    "  - Runs segmentation forward pass.\n",
    "  - Captures encoder features for classification.\n",
    "  - Computes segmentation loss (`DiceCELoss`) and classification loss (`BCEWithLogitsLoss`), combined with a weight (0.3).\n",
    "  - Computes segmentation metrics: predicted mask, confusion matrix, HD95.\n",
    "  - Updates model weights using AMP (Autocast + GradScaler).\n",
    "- Logs metrics per epoch: accuracy, recall, F1, HD95, QC rate, and total loss.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ 4. Validation Phase (Every `val_interval` epochs)\n",
    "\n",
    "- Sets model to `eval()` mode, disables gradient calculation.\n",
    "- For each batch in the validation set:\n",
    "  - Pads input to compatible dimensions (`pad_to_factor()`).\n",
    "  - Runs **sliding window inference** via `SlidingWindowInferer`.\n",
    "  - Crops back to original shape (`crop_to_shape()`).\n",
    "  - Extracts encoder features for classification.\n",
    "  - Computes validation loss, Dice score, HD95, and classification metrics.\n",
    "  - Updates QC statistics (`qc_val`).\n",
    "- Writes validation metrics to CSV.\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ 5. Checkpointing\n",
    "\n",
    "If validation Dice improves:\n",
    "- Saves model weights (`best.pt`) and encoder-only checkpoint (`encoder_fullstate.pt`).\n",
    "- Also saves current state to `last.pt` after every validation.\n",
    "\n",
    "Checkpoints include:\n",
    "- Epoch number\n",
    "- Model weights (`seg`, `cls`)\n",
    "- Optimizer and scaler states\n",
    "- Best validation Dice score\n",
    "- Paths to config and environment metadata\n",
    "\n",
    "---\n",
    "\n",
    "### üßπ 6. Cleanup and Finalization\n",
    "\n",
    "- Clears variables to free GPU memory after each epoch.\n",
    "- Logs total training time in minutes.\n",
    "- Closes the metrics CSV file safely.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This loop handles:\n",
    "- Dual-task training (segmentation + classification)\n",
    "- Accurate metric tracking and logging\n",
    "- Reproducible resume functionality\n",
    "- Intelligent model saving based on best performance\n",
    "- Efficient handling of 3D volumetric data (with padding + sliding window inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d12f12-067c-4511-a64d-58fb659ad3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val loops (refactored: correct metrics, single CSV writer, stable val Dice)\n",
    "\n",
    "# Assumes:\n",
    "# - start_epoch, end_epoch, best_val_dice defined (from resume block); if not, set them here.\n",
    "# - metrics_f, metrics_w already opened by write_csv_header with header:\n",
    "#   [\"epoch\",\"split\",\"loss\",\"dice\",\"acc\",\"recall\",\"f1\",\"hd95\",\"lr\",\"seconds\",\"qc_warns\",\"qc_total\",\"qc_rate\"]\n",
    "\n",
    "if \"start_epoch\" not in globals() or \"end_epoch\" not in globals():\n",
    "    EPOCHS = 500\n",
    "    start_epoch, end_epoch = 1, EPOCHS\n",
    "if \"best_val_dice\" not in globals():\n",
    "    best_val_dice = -1.0\n",
    "\n",
    "val_interval = 1\n",
    "\n",
    "t0_all = time.time()\n",
    "for epoch in range(start_epoch, end_epoch + 1):\n",
    "    t_epoch = time.time()\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    cm_train = new_cm()\n",
    "    seg_net.train(); cls_head.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_steps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # QC update per-sample (patch-level)\n",
    "        for b in decollate_batch(batch):\n",
    "            qc_train.update(int(b.get(\"qc_before_vox\", 0)), int(b.get(\"qc_after_vox\", 0)), str(b.get(\"case_id\", \"?\")))\n",
    "        batch = to_device(batch, device)\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"].long()\n",
    "        class_labels = batch[\"class_label\"].view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
    "            # segmentation forward\n",
    "            encoder_feat[\"x\"] = None\n",
    "            seg_logits = seg_net(images)                 # deep_supervision=False\n",
    "            seg_logits_main = seg_logits\n",
    "\n",
    "            # train segmentation metrics (per-batch)\n",
    "            y_pred_tr = torch.argmax(torch.softmax(seg_logits_main, dim=1), dim=1, keepdim=True)\n",
    "            update_cm(cm_train, y_pred_tr, labels)\n",
    "            hd95_tr(y_pred_tr, labels)\n",
    "\n",
    "            # classification forward\n",
    "            feat = encoder_feat[\"x\"] if encoder_feat[\"x\"] is not None else seg_logits_main\n",
    "            cls_logits = cls_head(feat)\n",
    "\n",
    "            # losses\n",
    "            loss_seg = seg_loss_fn(seg_logits_main, labels)\n",
    "            loss_cls = cls_loss_fn(cls_logits, class_labels)\n",
    "            loss = loss_seg + 0.3 * loss_cls\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_steps += 1\n",
    "\n",
    "    epoch_loss /= max(1, num_steps)\n",
    "    tr_acc, tr_rec, tr_f1 = cm_metrics(cm_train)\n",
    "    tr_hd95 = hd95_tr.aggregate().item(); hd95_tr.reset()\n",
    "    train_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    qc_rate_tr = (qc_train.warn / max(1, qc_train.total))\n",
    "\n",
    "    print(f\"Epoch {epoch}/{end_epoch} - train loss: {epoch_loss:.4f}\")\n",
    "    print(f\"  Train: acc={tr_acc:.4f} rec={tr_rec:.4f} f1={tr_f1:.4f} hd95={tr_hd95:.2f}\")\n",
    "    metrics_w.writerow([\n",
    "        epoch, \"train\",\n",
    "        f\"{epoch_loss:.6f}\", \"\", f\"{tr_acc:.6f}\", f\"{tr_rec:.6f}\", f\"{tr_f1:.6f}\", f\"{tr_hd95:.6f}\",\n",
    "        f\"{train_lr:.6g}\", f\"{(time.time()-t_epoch):.2f}\", qc_train.warn, qc_train.total, f\"{qc_rate_tr:.4f}\"\n",
    "    ]); metrics_f.flush()\n",
    "\n",
    "    # reset QC per-epoch if you prefer epoch-local stats\n",
    "    qc_train.total = qc_train.warn = 0\n",
    "    if epoch % val_interval == 0:\n",
    "        qc_train.summary()\n",
    "\n",
    "    # ---- VAL ----\n",
    "    if epoch % val_interval == 0:\n",
    "        cm_val = new_cm()\n",
    "        seg_net.eval(); cls_head.eval()\n",
    "        dice_metric.reset()\n",
    "        val_loss = 0.0\n",
    "        steps = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for b in decollate_batch(batch):\n",
    "                    qc_val.update(int(b.get(\"qc_before_vox\", 0)), int(b.get(\"qc_after_vox\", 0)), str(b.get(\"case_id\", \"?\")))\n",
    "                batch = to_device(batch, device)\n",
    "                images = batch[\"image\"]\n",
    "                labels = batch[\"label\"].long()\n",
    "                class_labels = batch[\"class_label\"].view(-1, 1)\n",
    "\n",
    "                with autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
    "                    # padded sliding-window seg\n",
    "                    images_p = pad_to_factor(images, factor=32)\n",
    "                    seg_logits = inferer(inputs=images_p, network=seg_net)\n",
    "\n",
    "                    # classification (populate encoder features on same padded grid)\n",
    "                    encoder_feat[\"x\"] = None\n",
    "                    _ = seg_net(images_p)\n",
    "                    if seg_logits.shape[-3:] != labels.shape[-3:]:\n",
    "                        seg_logits = crop_to_shape(seg_logits, labels.shape[-3:])\n",
    "\n",
    "                    feat = encoder_feat[\"x\"] if encoder_feat[\"x\"] is not None else seg_logits\n",
    "                    cls_logits = cls_head(feat)\n",
    "\n",
    "                    # per-batch seg metrics\n",
    "                    y_pred = torch.argmax(torch.softmax(seg_logits, dim=1), dim=1, keepdim=True)\n",
    "                    dice_metric(y_pred=y_pred, y=labels)\n",
    "                    update_cm(cm_val, y_pred, labels)\n",
    "                    hd95_val(y_pred, labels)\n",
    "\n",
    "                    # per-batch val loss\n",
    "                    loss_seg = seg_loss_fn(seg_logits, labels)\n",
    "                    loss_cls = cls_loss_fn(cls_logits, class_labels)\n",
    "                    loss = loss_seg + 0.3 * loss_cls\n",
    "                val_loss += loss.item()\n",
    "                steps += 1\n",
    "\n",
    "        mean_dice = dice_metric.aggregate().item(); dice_metric.reset()\n",
    "        val_loss /= max(1, steps)\n",
    "        vl_acc, vl_rec, vl_f1 = cm_metrics(cm_val)\n",
    "        vl_hd95 = hd95_val.aggregate().item(); hd95_val.reset()\n",
    "        qc_rate_val = (qc_val.warn / max(1, qc_val.total))\n",
    "\n",
    "        print(f\"  Val loss: {val_loss:.4f} | Val Dice(tumor): {mean_dice:.4f}\")\n",
    "        print(f\"  Val  : acc={vl_acc:.4f} rec={vl_rec:.4f} f1={vl_f1:.4f} hd95={vl_hd95:.2f}\")\n",
    "        qc_val.summary()\n",
    "\n",
    "        metrics_w.writerow([\n",
    "            epoch, \"val\",\n",
    "            f\"{val_loss:.6f}\", f\"{mean_dice:.6f}\", f\"{vl_acc:.6f}\", f\"{vl_rec:.6f}\", f\"{vl_f1:.6f}\", f\"{vl_hd95:.6f}\",\n",
    "            f\"{train_lr:.6g}\", f\"{(time.time()-t_epoch):.2f}\", qc_val.warn, qc_val.total, f\"{qc_rate_val:.4f}\"\n",
    "        ]); metrics_f.flush()\n",
    "        qc_val.total = qc_val.warn = 0\n",
    "\n",
    "        # ---- Checkpointing ----\n",
    "        def save_ckpt(path: Path):\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"seg\": seg_net.state_dict(),\n",
    "                \"cls\": cls_head.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scaler\": scaler.state_dict(),\n",
    "                \"best_val_dice\": best_val_dice,\n",
    "                \"config_path\": str(CONFIG_JSON) if \"CONFIG_JSON\" in globals() else None,\n",
    "                \"env_path\": str(ENV_JSON) if \"ENV_JSON\" in globals() else None,\n",
    "            }, path)\n",
    "\n",
    "        run_last = (RUN_DIR if \"RUN_DIR\" in globals() else ckpt_dir) / \"last.pt\"\n",
    "        save_ckpt(run_last)\n",
    "\n",
    "        if mean_dice > best_val_dice:\n",
    "            best_val_dice = mean_dice\n",
    "            run_best = (RUN_DIR if \"RUN_DIR\" in globals() else ckpt_dir) / \"best.pt\"\n",
    "            save_ckpt(run_best)\n",
    "            # convenience copy\n",
    "            save_ckpt(ckpt_dir / \"best.pt\")\n",
    "            print(f\"  [Saved] best.pt with Dice {best_val_dice:.4f}\")\n",
    "\n",
    "        # encoder-only (save full state; downstream can load encoder subset)\n",
    "        torch.save({\"seg_encoder_compatible\": True, \"state_dict\": seg_net.state_dict()},\n",
    "                   (RUN_DIR if \"RUN_DIR\" in globals() else ckpt_dir) / \"encoder_fullstate.pt\")\n",
    "\n",
    "        # Cleanup\n",
    "        try:\n",
    "            del images, labels, seg_logits, cls_logits, feat, y_pred\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del images_p\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Training done in {(time.time()-t0_all)/60:.1f} min\")\n",
    "# Close the metrics file (kept open for speed)\n",
    "try:\n",
    "    metrics_f.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc30a8-7c84-4425-809f-ae7490f0fc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fe8f0-696c-46a7-9732-73cd7fdca965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b4bd0-a042-491b-bfd6-fb3a1adfe03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee24f60-3f43-40d0-b13c-a4af7ff9f9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44802171-9ab8-4e46-b0ca-f33face35c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a96009-2bd2-4cab-806f-1355e6c06530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb110d-98d5-48d7-a9bf-dc9cb7938994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab079a5-6d21-4bcc-8048-77ae56679f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d12997-0d27-4e54-8b8c-7e0ae90bc1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34372185-ffd9-4437-8754-51a2ba6055e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38b77d-f1cf-44d0-8632-88c26160fdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87cbd4-bda6-4e76-8fe1-13e39b626513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dbda3-5ac2-472f-b0a7-5b444730f107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707c157-5c12-4fa5-9c91-cbe21a7c8137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e7927-a39c-4833-a570-abbb60f162eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa65f6f-e6f8-4d58-b019-17e3c6de9196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad969a-d726-492c-82ca-8ee38baa325b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
