{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a722de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files updated with correct paths\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to fix the file paths\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "train_csv = pd.read_csv('derived/unified_dualtask/train.csv')\n",
    "val_csv = pd.read_csv('derived/unified_dualtask/val.csv')\n",
    "test_csv = pd.read_csv('derived/unified_dualtask/test.csv')\n",
    "\n",
    "# Function to fix file paths\n",
    "def fix_file_paths(df):\n",
    "    # Replace the old path with your current path\n",
    "    df['image_path'] = df['image_path'].str.replace(\n",
    "        '/Users/chufal/projects/DHAI-Brain-Segmentation',\n",
    "        '/home/qarc/projects/DHAI-Brain-Segmentation'\n",
    "    )\n",
    "    df['label_path'] = df['label_path'].str.replace(\n",
    "        '/Users/chufal/projects/DHAI-Brain-Segmentation',\n",
    "        '/home/qarc/projects/DHAI-Brain-Segmentation'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Fix all CSV files\n",
    "train_csv = fix_file_paths(train_csv)\n",
    "val_csv = fix_file_paths(val_csv)\n",
    "test_csv = fix_file_paths(test_csv)\n",
    "\n",
    "# Save the corrected CSV files\n",
    "train_csv.to_csv('derived/unified_dualtask/train_fixed.csv', index=False)\n",
    "val_csv.to_csv('derived/unified_dualtask/val_fixed.csv', index=False)\n",
    "test_csv.to_csv('derived/unified_dualtask/test_fixed.csv', index=False)\n",
    "\n",
    "print(\"CSV files updated with correct paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb7c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJ_ROOT = Path('/home/qarc/projects/DHAI-Brain-Segmentation')\n",
    "DUALTASK_ROOT = PROJ_ROOT / 'derived' / 'unified_dualtask'\n",
    "TRAIN_CSV = DUALTASK_ROOT / 'train_fixed.csv'\n",
    "VAL_CSV = DUALTASK_ROOT / 'val_fixed.csv'\n",
    "TEST_CSV = DUALTASK_ROOT / 'test_fixed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ec9aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking /home/qarc/projects/DHAI-Brain-Segmentation/derived/unified_dualtask/train_fixed.csv:\n",
      "All files exist!\n",
      "Checking /home/qarc/projects/DHAI-Brain-Segmentation/derived/unified_dualtask/val_fixed.csv:\n",
      "All files exist!\n",
      "Checking /home/qarc/projects/DHAI-Brain-Segmentation/derived/unified_dualtask/test_fixed.csv:\n",
      "All files exist!\n",
      "\n",
      "✅ All files validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Validation cell - check file existence\n",
    "import os\n",
    "\n",
    "def validate_file_paths(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Checking {csv_path}:\")\n",
    "    \n",
    "    missing_files = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if not os.path.exists(row['image_path']):\n",
    "            missing_files.append(f\"Image: {row['image_path']}\")\n",
    "        if not os.path.exists(row['label_path']):\n",
    "            missing_files.append(f\"Label: {row['label_path']}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"Missing {len(missing_files)} files:\")\n",
    "        for missing in missing_files[:5]:  # Show first 5\n",
    "            print(f\"  - {missing}\")\n",
    "        if len(missing_files) > 5:\n",
    "            print(f\"  ... and {len(missing_files) - 5} more\")\n",
    "    else:\n",
    "        print(\"All files exist!\")\n",
    "    \n",
    "    return len(missing_files) == 0\n",
    "\n",
    "# Check all CSV files\n",
    "train_valid = validate_file_paths(TRAIN_CSV)\n",
    "val_valid = validate_file_paths(VAL_CSV)\n",
    "test_valid = validate_file_paths(TEST_CSV)\n",
    "\n",
    "if train_valid and val_valid and test_valid:\n",
    "    print(\"\\n✅ All files validated successfully!\")\n",
    "else:\n",
    "    print(\"\\n❌ Some files are missing. Please fix the paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74bb2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLASS DISTRIBUTION ANALYSIS ===\n",
      "Training set:\n",
      "  Class 0 (No tumor): 647 samples\n",
      "  Class 1 (Tumor): 325 samples\n",
      "  Total: 972 samples\n",
      "  Positive ratio: 33.4%\n",
      "\n",
      "Validation set:\n",
      "  Class 0 (No tumor): 139 samples\n",
      "  Class 1 (Tumor): 70 samples\n",
      "  Total: 209 samples\n",
      "  Positive ratio: 33.5%\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in your dataset\n",
    "def analyze_class_distribution():\n",
    "    print(\"=== CLASS DISTRIBUTION ANALYSIS ===\")\n",
    "    \n",
    "    # Training set\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    train_counts = train_df['class_label'].value_counts().sort_index()\n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  Class 0 (No tumor): {train_counts.get(0, 0)} samples\")\n",
    "    print(f\"  Class 1 (Tumor): {train_counts.get(1, 0)} samples\")\n",
    "    print(f\"  Total: {len(train_df)} samples\")\n",
    "    print(f\"  Positive ratio: {train_counts.get(1, 0)/len(train_df)*100:.1f}%\")\n",
    "    \n",
    "    # Validation set\n",
    "    val_df = pd.read_csv(VAL_CSV)\n",
    "    val_counts = val_df['class_label'].value_counts().sort_index()\n",
    "    print(f\"\\nValidation set:\")\n",
    "    print(f\"  Class 0 (No tumor): {val_counts.get(0, 0)} samples\")\n",
    "    print(f\"  Class 1 (Tumor): {val_counts.get(1, 0)} samples\")\n",
    "    print(f\"  Total: {len(val_df)} samples\")\n",
    "    print(f\"  Positive ratio: {val_counts.get(1, 0)/len(val_df)*100:.1f}%\")\n",
    "    \n",
    "    # Check if this is a severe imbalance\n",
    "    if train_counts.get(1, 0) / len(train_df) < 0.1:\n",
    "        print(\"\\n⚠️  SEVERE CLASS IMBALANCE DETECTED!\")\n",
    "        print(\"   This will cause training issues. Consider:\")\n",
    "        print(\"   1. Class weighting in loss function\")\n",
    "        print(\"   2. Data augmentation for minority class\")\n",
    "        print(\"   3. Balanced sampling strategies\")\n",
    "\n",
    "analyze_class_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67952036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79c721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44156f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15659fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5e5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
