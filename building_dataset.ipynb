{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8798a8",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb7e366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0c357",
   "metadata": {},
   "source": [
    "# ----- CONFIG: set your paths and task info ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8edabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG: set your paths and task info ----------------\n",
    "# Target nnU-Net raw task\n",
    "TASK_ID = 100\n",
    "TASK_NAME = \"DHAI_T1cEnhancingSeg\"\n",
    "TASK_DIR = Path(f\"/Users/chufal/nnUNet_raw/Task{TASK_ID:03d}_{TASK_NAME}\")  # change if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31fb5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data roots\n",
    "ROOTS = {\n",
    "    \"MU\": \"/Users/chufal/projects/Datasets/PKG-MU-Glioma-Post/MU-Glioma-Post\",\n",
    "    \"MET\": \"/Users/chufal/projects/Datasets/PKG-Pretreat-MetsToBrain-Masks/Pretreat-MetsToBrain-Masks\",\n",
    "    \"BCBM\": \"/Users/chufal/projects/Datasets/PKG-BCBM-RadioGenomics_Images_Masks_Dec2024/BCBM_KSC_curated_data\",\n",
    "    \"UCSD\": \"/Users/chufal/projects/Datasets/PKG-UCSD-PTGBM-v1/UCSD-PTGBM\",\n",
    "    \"UCSF\": \"/Users/chufal/projects/Datasets/PKG - UCSF-PDGM Version 5/UCSF-PDGM-v5\",\n",
    "    \"UPENN\": \"/Users/chufal/projects/Datasets/PKG-UPENN-GBM-NIfTI/UPENN-GBM/NIfTI-files\",\n",
    "    \"BRATSAfrica\": \"/Users/chufal/projects/Datasets/PKG-BraTS-Africa/BraTS-Africa\",\n",
    "    # \"YALE\": excluded (no segmentations)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9b8bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhancing-label overrides per dataset when masks are multi-class.\n",
    "# If None, the code tries auto-detect: prefer 4, else 3, else 1 (BraTS-like patterns).\n",
    "ENHANCING_LABEL_OVERRIDES: Dict[str, Optional[List[int]]] = {\n",
    "    \"MU\": [3],      # FeTS label 3 = Enhancing Tissue (ET)\n",
    "    \"UCSF\": None,    # likely multi-class; auto-detect per-case\n",
    "    \"UPENN\": None,   # often binary; if multi-class, auto-detect\n",
    "    \"MET\": [1],      # MET segs typically binary -> >0; setting [1] makes intent explicit\n",
    "    \"UCSD\": [1],     # \"_enhancing_cellular_tumor_seg\" is binary -> >0\n",
    "    \"BCBM\": [1],     # union of region masks -> binary\n",
    "    \"BRATSAfrica\": [3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fadf4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staging options\n",
    "WRITE_FILES = False        # set True to actually write files and dataset.json\n",
    "LINK_METHOD = \"symlink\"    # \"symlink\" or \"copy\"\n",
    "TRAIN_TEST_SPLIT = 0.2     # test fraction per dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b4754",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34491bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unique_values(mask_path: str, max_vox: int = 2_000_000) -> List[int]:\n",
    "    img = nib.load(mask_path)\n",
    "    arr = np.asanyarray(img.dataobj)\n",
    "    if arr.size > max_vox:\n",
    "        # random subsample for speed\n",
    "        rng = np.random.default_rng(13)\n",
    "        idx = rng.integers(0, arr.size, size=max_vox)\n",
    "        uniq = np.unique(arr.reshape(-1)[idx])\n",
    "    else:\n",
    "        uniq = np.unique(arr)\n",
    "    # int-like rounding if float labels\n",
    "    if arr.dtype.kind not in (\"i\",\"u\"):\n",
    "        uniq = np.unique(np.round(uniq).astype(np.int32))\n",
    "    return [int(u) for u in uniq.tolist()]\n",
    "\n",
    "def pick_enhancing_labels(dataset_key: str, mask_path: str) -> List[int]:\n",
    "    override = ENHANCING_LABEL_OVERRIDES.get(dataset_key)\n",
    "    if override:\n",
    "        return override\n",
    "    uniq = load_unique_values(mask_path)\n",
    "    uniq_nz = [u for u in uniq if u != 0]\n",
    "    if not uniq_nz:\n",
    "        return []\n",
    "    # Prefer 3 (ET), else 1; NEVER pick 4 (RC)\n",
    "    if 3 in uniq_nz:\n",
    "        return [3]\n",
    "    if 1 in uniq_nz:\n",
    "        return [1]\n",
    "    # if nothing matches heuristics, fallback to all non-zero but exclude 4\n",
    "    return [u for u in uniq_nz if u != 4]\n",
    "\n",
    "def mask_has_any_labels(mask_path: str, pos_labels: list[int]) -> bool:\n",
    "    img = nib.load(mask_path)\n",
    "    arr = np.asanyarray(img.dataobj)\n",
    "    if arr.dtype.kind not in (\"i\",\"u\"):\n",
    "        arr = np.round(arr).astype(np.int32)\n",
    "    return np.isin(arr, pos_labels).any()\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def stage_file(src: Path, dst: Path):\n",
    "    ensure_dir(dst.parent)\n",
    "    if dst.exists():\n",
    "        return\n",
    "    if LINK_METHOD == \"symlink\":\n",
    "        os.symlink(src, dst)\n",
    "    else:\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "def write_binary_label_from_multiclass(src_mask: Path, dst_mask: Path, pos_labels: List[int]):\n",
    "    ensure_dir(dst_mask.parent)\n",
    "    img = nib.load(str(src_mask))\n",
    "    data = np.asanyarray(img.dataobj)\n",
    "    if data.dtype.kind not in (\"i\",\"u\"):\n",
    "        data = np.round(data).astype(np.int32)\n",
    "    binmask = np.isin(data, pos_labels).astype(np.uint8)\n",
    "    out = nib.Nifti1Image(binmask, affine=img.affine, header=img.header)\n",
    "    out.header.set_data_dtype(np.uint8)\n",
    "    nib.save(out, str(dst_mask))\n",
    "\n",
    "# 4) OPTIONAL: make label writer robust to rare 4D one-hot segmentations\n",
    "def write_binary_label_from_multiclass(src_mask: Path, dst_mask: Path, pos_labels: list[int]):\n",
    "    ensure_dir(dst_mask.parent)\n",
    "    img = nib.load(str(src_mask))\n",
    "    data = np.asanyarray(img.dataobj)\n",
    "    # If 4D one-hot, convert to label map first\n",
    "    if data.ndim == 4 and data.shape[-1] <= 10 and np.array_equal(np.unique(data), [0, 1]):\n",
    "        data = np.argmax(data, axis=-1)\n",
    "    if data.dtype.kind not in (\"i\",\"u\"):\n",
    "        data = np.round(data).astype(np.int32)\n",
    "    binmask = np.isin(data, pos_labels).astype(np.uint8)\n",
    "    out = nib.Nifti1Image(binmask, affine=img.affine, header=img.header)\n",
    "    out.header.set_data_dtype(np.uint8)\n",
    "    nib.save(out, str(dst_mask))\n",
    "\n",
    "# ---------------- Dataset-specific discoverers ----------------\n",
    "def discover_MET() -> List[dict]:\n",
    "    root = Path(ROOTS[\"MET\"])\n",
    "    out = []\n",
    "    for case in sorted([d for d in root.iterdir() if d.is_dir() and d.name.startswith(\"BraTS-MET-\")]):\n",
    "        t1c = next((case/f for f in os.listdir(case) if f.endswith(\"-t1c.nii.gz\")), None)\n",
    "        seg = next((case/f for f in os.listdir(case) if f.endswith(\"-seg.nii.gz\")), None)\n",
    "        if t1c and seg:\n",
    "            out.append({\"dataset\":\"MET\",\"id\":case.name,\"image\":str(t1c),\"mask\":str(seg),\"mask_kind\":\"binary\"})\n",
    "    return out\n",
    "\n",
    "def discover_BCBM() -> list[dict]:\n",
    "    root = Path(ROOTS[\"BCBM\"])\n",
    "    out: list[dict] = []\n",
    "    if not root.is_dir():\n",
    "        print(f\"[WARN] Curated BCBM root not found: {root}\")\n",
    "        return out\n",
    "\n",
    "    def pick_image(case_dir: Path) -> Path | None:\n",
    "        # Preferred file name pattern\n",
    "        preferred = [f for f in case_dir.iterdir()\n",
    "                     if f.is_file() and f.name.endswith(\"_image_ss_n4.nii.gz\")]\n",
    "        if preferred:\n",
    "            return preferred[0]\n",
    "        # Fallback: first non-mask nii.gz\n",
    "        others = [f for f in case_dir.iterdir()\n",
    "                  if f.is_file() and f.name.endswith(\".nii.gz\") and \"_mask_\" not in f.name]\n",
    "        return others[0] if others else None\n",
    "\n",
    "    for case in sorted([d for d in root.iterdir() if d.is_dir() and not d.name.startswith(\".\")]):\n",
    "        image = pick_image(case)\n",
    "        mask_files = [f for f in case.iterdir()\n",
    "                      if f.is_file() and f.name.endswith(\".nii.gz\") and \"_mask_\" in f.name]\n",
    "        if image is None:\n",
    "            print(f\"[SKIP] {case.name}: no MRI found\")\n",
    "            continue\n",
    "        if not mask_files:\n",
    "            print(f\"[SKIP] {case.name}: no masks in curated folder\")\n",
    "            continue\n",
    "        out.append({\n",
    "            \"dataset\": \"BCBM\",\n",
    "            \"id\": case.name,\n",
    "            \"image\": str(image),\n",
    "            \"mask_files\": [str(m) for m in sorted(mask_files)],\n",
    "            \"mask_kind\": \"multi_file_union\"\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def discover_UCSD() -> List[dict]:\n",
    "    root = Path(ROOTS[\"UCSD\"])\n",
    "    out = []\n",
    "    for case in sorted([d for d in root.iterdir() if d.is_dir()]):\n",
    "        t1post = next((d for d in case.iterdir() if d.name.endswith(\"_T1post.nii.gz\")), None)\n",
    "        enh = next((d for d in case.iterdir() if d.name.endswith(\"_enhancing_cellular_tumor_seg.nii.gz\")), None)\n",
    "        if t1post and enh:\n",
    "            out.append({\"dataset\":\"UCSD\",\"id\":case.name,\"image\":str(t1post),\"mask\":str(enh),\"mask_kind\":\"binary\"})\n",
    "    return out\n",
    "\n",
    "def discover_UCSF() -> List[dict]:\n",
    "    root = Path(ROOTS[\"UCSF\"])\n",
    "    out = []\n",
    "    for case in sorted([d for d in root.iterdir() if d.is_dir()]):\n",
    "        t1c = next((d for d in case.iterdir() if d.name.endswith(\"_T1c_bias.nii.gz\")), None)\n",
    "        if t1c is None:\n",
    "            t1c = next((d for d in case.iterdir() if d.name.endswith(\"_T1c.nii.gz\")), None)\n",
    "        seg = next((d for d in case.iterdir() if d.name.endswith(\"_tumor_segmentation.nii.gz\")), None)\n",
    "        if t1c and seg:\n",
    "            out.append({\"dataset\":\"UCSF\",\"id\":case.name,\"image\":str(t1c),\"mask\":str(seg),\"mask_kind\":\"multiclass\"})\n",
    "    return out\n",
    "\n",
    "def discover_UPENN() -> List[dict]:\n",
    "    root = Path(ROOTS[\"UPENN\"])\n",
    "    img_dir = Path(root) / \"images_structural_unstripped\"\n",
    "    seg_dir = Path(root) / \"images_segm\"\n",
    "    if not img_dir.is_dir():\n",
    "        return []\n",
    "    seg_map = {}\n",
    "    if seg_dir.is_dir():\n",
    "        for f in seg_dir.iterdir():\n",
    "            if f.name.endswith(\"_segm.nii.gz\"):\n",
    "                key = f.name.replace(\"_segm.nii.gz\",\"\")\n",
    "                seg_map[key] = f\n",
    "    out = []\n",
    "    for case in sorted([d for d in img_dir.iterdir() if d.is_dir()]):\n",
    "        t1gd = next((f for f in case.iterdir() if f.name.endswith(\"_T1GD_unstripped.nii.gz\")), None)\n",
    "        if not t1gd:\n",
    "            continue\n",
    "        key = case.name  # matches seg key if present\n",
    "        seg = seg_map.get(key)\n",
    "        if seg:\n",
    "            out.append({\"dataset\":\"UPENN\",\"id\":case.name,\"image\":str(t1gd),\"mask\":str(seg),\"mask_kind\":\"auto\"})  # auto: detect binary/multi\n",
    "    return out\n",
    "\n",
    "def discover_MU() -> List[dict]:\n",
    "    root = Path(ROOTS[\"MU\"])\n",
    "    out = []\n",
    "    for patient in sorted([d for d in root.iterdir() if d.is_dir()]):\n",
    "        for tp in sorted([d for d in patient.iterdir() if d.is_dir()]):\n",
    "            files = [f for f in tp.iterdir() if f.is_file() and f.name.endswith(\".nii.gz\")]\n",
    "            t1c = next((f for f in files if \"brain_t1c\" in f.name), None)\n",
    "            seg = next((f for f in files if f.name.endswith(\"tumorMask.nii.gz\")), None)\n",
    "            if t1c and seg:\n",
    "                out.append({\"dataset\":\"MU\",\"id\":f\"{patient.name}_{tp.name}\",\"image\":str(t1c),\"mask\":str(seg),\"mask_kind\":\"multiclass\"})\n",
    "    return out\n",
    "\n",
    "# 3) Discover BraTS-Africa (only 95_Glioma; ignore 51_OtherNeoplasms)\n",
    "def discover_BRATSAfrica() -> list[dict]:\n",
    "    root = Path(ROOTS[\"BRATSAfrica\"])\n",
    "    glioma_dir = root / \"95_Glioma\"\n",
    "    out = []\n",
    "    if not glioma_dir.is_dir():\n",
    "        return out\n",
    "    for case in sorted([d for d in glioma_dir.iterdir() if d.is_dir() and not d.name.startswith(\".\")]):\n",
    "        t1c = next((case/f for f in os.listdir(case) if f.endswith(\"-t1c.nii.gz\")), None)\n",
    "        seg = next((case/f for f in os.listdir(case) if f.endswith(\"-seg.nii.gz\")), None)\n",
    "        if t1c and seg:\n",
    "            out.append({\n",
    "                \"dataset\": \"BRATSAfrica\",\n",
    "                \"id\": case.name,\n",
    "                \"image\": str(t1c),\n",
    "                \"mask\": str(seg),\n",
    "                \"mask_kind\": \"multiclass\"  # label map with ET=3\n",
    "            })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2439c4",
   "metadata": {},
   "source": [
    "# --- Collect candidates ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4a78d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nonempty_nifti(path: str) -> bool:\n",
    "    p = Path(path)\n",
    "    try:\n",
    "        if not (p.is_file() and p.stat().st_size > 0):\n",
    "            return False\n",
    "        # fast header sniff without reading full data\n",
    "        _ = nib.load(str(p))  # will throw for corrupt/empty headers\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_unique_values(mask_path: str, max_vox: int = 2_000_000) -> list[int]:\n",
    "    p = Path(mask_path)\n",
    "    if not is_nonempty_nifti(str(p)):\n",
    "        raise RuntimeError(f\"Mask not readable or empty: {mask_path}\")\n",
    "    try:\n",
    "        img = nib.load(str(p))\n",
    "        arr = np.asanyarray(img.dataobj)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read mask {mask_path}: {e}\") from e\n",
    "\n",
    "    if arr.size > max_vox:\n",
    "        rng = np.random.default_rng(13)\n",
    "        idx = rng.integers(0, arr.size, size=max_vox)\n",
    "        uniq = np.unique(arr.reshape(-1)[idx])\n",
    "    else:\n",
    "        uniq = np.unique(arr)\n",
    "    if arr.dtype.kind not in (\"i\",\"u\"):\n",
    "        uniq = np.unique(np.round(uniq).astype(np.int32))\n",
    "    return [int(u) for u in uniq.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8be30dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_candidates():\n",
    "    all_items = []\n",
    "    all_items += discover_MET()\n",
    "    all_items += discover_BCBM()\n",
    "    all_items += discover_UCSD()\n",
    "    all_items += discover_UCSF()\n",
    "    all_items += discover_UPENN()\n",
    "    all_items += discover_MU()\n",
    "    all_items += discover_BRATSAfrica()  # <- add here\n",
    "\n",
    "    ok = []\n",
    "    bad_files = []\n",
    "    for it in all_items:\n",
    "        img_ok = is_nonempty_nifti(it[\"image\"])\n",
    "        if not img_ok:\n",
    "            bad_files.append((\"image\", it[\"dataset\"], it.get(\"id\"), it[\"image\"]))\n",
    "            continue\n",
    "\n",
    "        # one mask file\n",
    "        if it.get(\"mask\"):\n",
    "            if not is_nonempty_nifti(it[\"mask\"]):\n",
    "                bad_files.append((\"mask\", it[\"dataset\"], it.get(\"id\"), it[\"mask\"]))\n",
    "                continue\n",
    "\n",
    "        # multiple mask files (union case)\n",
    "        if it.get(\"mask_files\"):\n",
    "            mask_ok = True\n",
    "            for p in it[\"mask_files\"]:\n",
    "                if not is_nonempty_nifti(p):\n",
    "                    bad_files.append((\"mask\", it[\"dataset\"], it.get(\"id\"), p))\n",
    "                    mask_ok = False\n",
    "            if not mask_ok:\n",
    "                continue\n",
    "\n",
    "        ok.append(it)\n",
    "\n",
    "    if bad_files:\n",
    "        print(f\"[WARN] Skipping {len(bad_files)} unreadable/empty files. Examples:\")\n",
    "        for kind, ds, cid, p in bad_files[:10]:\n",
    "            print(f\"  - {kind} | {ds} | {cid} | {p}\")\n",
    "    return ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58fc7eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Skipping 2 unreadable/empty files. Examples:\n",
      "  - mask | UCSF | UCSF-PDGM-0541_nifti | /Users/chufal/projects/Datasets/PKG - UCSF-PDGM Version 5/UCSF-PDGM-v5/UCSF-PDGM-0541_nifti/UCSF-PDGM-0541_tumor_segmentation.nii.gz\n",
      "  - mask | MU | PatientID_0275_Timepoint_6 | /Users/chufal/projects/Datasets/PKG-MU-Glioma-Post/MU-Glioma-Post/PatientID_0275/Timepoint_6/PatientID_0275_Timepoint_6_tumorMask.nii.gz\n",
      "Found candidates: 1983 by dataset → {'BCBM': 264, 'BRATSAfrica': 95, 'MET': 200, 'MU': 593, 'UCSD': 184, 'UCSF': 500, 'UPENN': 147}\n"
     ]
    }
   ],
   "source": [
    "items = collect_candidates()\n",
    "print(f\"Found candidates: {len(items)} by dataset →\",\n",
    "      {k: sum(1 for it in items if it['dataset']==k) for k in sorted(set(it['dataset'] for it in items))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "256a59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_and_generate(items: list[dict]):\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "\n",
    "    def nifti_readable(path: str) -> bool:\n",
    "        p = Path(path)\n",
    "        try:\n",
    "            if not (p.is_file() and p.stat().st_size > 0):\n",
    "                return False\n",
    "            _ = nib.load(str(p))\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def mask_has_any_labels(mask_path: str, pos_labels: list[int]) -> bool:\n",
    "        img = nib.load(mask_path)\n",
    "        arr = np.asanyarray(img.dataobj)\n",
    "        if arr.dtype.kind not in (\"i\", \"u\"):\n",
    "            arr = np.round(arr).astype(np.int32)\n",
    "        return np.isin(arr, pos_labels).any()\n",
    "\n",
    "    imagesTr = TASK_DIR / \"imagesTr\"\n",
    "    labelsTr = TASK_DIR / \"labelsTr\"\n",
    "    ensure_dir(imagesTr); ensure_dir(labelsTr)\n",
    "\n",
    "    training_entries = []\n",
    "    meta_rows = []\n",
    "\n",
    "    for idx, it in enumerate(items):\n",
    "        ds = it[\"dataset\"]\n",
    "        case_id = f\"{ds}__{it['id']}\"\n",
    "        img_src = Path(it[\"image\"])\n",
    "        img_dst = imagesTr / f\"{case_id}_0000.nii.gz\"\n",
    "        lbl_dst = labelsTr / f\"{case_id}.nii.gz\"\n",
    "\n",
    "        # Validate image first\n",
    "        if not nifti_readable(str(img_src)):\n",
    "            print(f\"[SKIP] {case_id}: image unreadable/empty → {img_src}\")\n",
    "            continue\n",
    "\n",
    "        mk = it[\"mask_kind\"]\n",
    "        try:\n",
    "            if mk == \"binary\":\n",
    "                mask_src = Path(it[\"mask\"])\n",
    "                if not nifti_readable(str(mask_src)):\n",
    "                    print(f\"[SKIP] {case_id}: mask unreadable/empty → {mask_src}\")\n",
    "                    continue\n",
    "                # Require any positive voxels\n",
    "                if not mask_has_any_labels(str(mask_src), [1]):\n",
    "                    print(f\"[SKIP] {case_id}: binary mask has zero positive voxels\")\n",
    "                    continue\n",
    "                if WRITE_FILES:\n",
    "                    stage_file(img_src, img_dst)\n",
    "                    write_binary_label_from_multiclass(mask_src, lbl_dst, pos_labels=[1])\n",
    "                training_entries.append({\"image\": f\"./imagesTr/{img_dst.name}\", \"label\": f\"./labelsTr/{lbl_dst.name}\"})\n",
    "\n",
    "            elif mk == \"multiclass\":\n",
    "                mask_src = Path(it[\"mask\"])\n",
    "                if not nifti_readable(str(mask_src)):\n",
    "                    print(f\"[SKIP] {case_id}: mask unreadable/empty → {mask_src}\")\n",
    "                    continue\n",
    "                pos = pick_enhancing_labels(ds, str(mask_src))\n",
    "                if not pos:\n",
    "                    print(f\"[SKIP] {case_id}: no enhancing labels determined\")\n",
    "                    continue\n",
    "                if not mask_has_any_labels(str(mask_src), pos):\n",
    "                    print(f\"[SKIP] {case_id}: no voxels for enhancing labels {pos}\")\n",
    "                    continue\n",
    "                if WRITE_FILES:\n",
    "                    stage_file(img_src, img_dst)\n",
    "                    write_binary_label_from_multiclass(mask_src, lbl_dst, pos_labels=pos)\n",
    "                training_entries.append({\"image\": f\"./imagesTr/{img_dst.name}\", \"label\": f\"./labelsTr/{lbl_dst.name}\"})\n",
    "\n",
    "            elif mk == \"auto\":\n",
    "                mask_src = Path(it[\"mask\"])\n",
    "                if not nifti_readable(str(mask_src)):\n",
    "                    print(f\"[SKIP] {case_id}: mask unreadable/empty → {mask_src}\")\n",
    "                    continue\n",
    "                uniq = load_unique_values(str(mask_src))\n",
    "                pos = [1] if set(uniq).issubset({0, 1}) else pick_enhancing_labels(ds, str(mask_src))\n",
    "                if not pos:\n",
    "                    print(f\"[SKIP] {case_id}: cannot determine enhancing labels from {uniq}\")\n",
    "                    continue\n",
    "                if not mask_has_any_labels(str(mask_src), pos):\n",
    "                    print(f\"[SKIP] {case_id}: no voxels for enhancing labels {pos}\")\n",
    "                    continue\n",
    "                if WRITE_FILES:\n",
    "                    stage_file(img_src, img_dst)\n",
    "                    write_binary_label_from_multiclass(mask_src, lbl_dst, pos_labels=pos)\n",
    "                training_entries.append({\"image\": f\"./imagesTr/{img_dst.name}\", \"label\": f\"./labelsTr/{lbl_dst.name}\"})\n",
    "\n",
    "            elif mk == \"multi_file_union\":\n",
    "                mask_files = [Path(p) for p in it[\"mask_files\"]]\n",
    "                all_readable = True\n",
    "                for mp in mask_files:\n",
    "                    if not nifti_readable(str(mp)):\n",
    "                        print(f\"[SKIP] {case_id}: union mask unreadable/empty → {mp}\")\n",
    "                        all_readable = False\n",
    "                        break\n",
    "                if not all_readable:\n",
    "                    continue\n",
    "                # Build union and verify non-empty\n",
    "                img_ref = nib.load(str(mask_files[0]))\n",
    "                union = None\n",
    "                for mp in mask_files:\n",
    "                    arr = np.asanyarray(nib.load(str(mp)).dataobj)\n",
    "                    union = (arr > 0) if union is None else (union | (arr > 0))\n",
    "                if not np.any(union):\n",
    "                    print(f\"[SKIP] {case_id}: union of masks is empty\")\n",
    "                    continue\n",
    "                if WRITE_FILES:\n",
    "                    stage_file(img_src, img_dst)\n",
    "                    union = union.astype(np.uint8)\n",
    "                    out = nib.Nifti1Image(union, affine=img_ref.affine, header=img_ref.header)\n",
    "                    out.header.set_data_dtype(np.uint8)\n",
    "                    nib.save(out, str(lbl_dst))\n",
    "                training_entries.append({\"image\": f\"./imagesTr/{img_dst.name}\", \"label\": f\"./labelsTr/{lbl_dst.name}\"})\n",
    "\n",
    "            else:\n",
    "                print(f\"[WARN] {case_id}: unknown mask_kind={mk}; skipping\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] {case_id}: error while preparing → {e}\")\n",
    "            continue\n",
    "\n",
    "        meta_rows.append({\"case\": case_id, \"dataset\": ds})\n",
    "\n",
    "    # simple per-dataset split\n",
    "    rng_seed = 42\n",
    "    datasets = np.array([r[\"dataset\"] for r in meta_rows])\n",
    "    cases = np.array([r[\"case\"] for r in meta_rows])\n",
    "    train_cases, test_cases = [], []\n",
    "    for ds_unique in sorted(set(datasets)):\n",
    "        ds_cases = cases[datasets == ds_unique]\n",
    "        n_test = max(1, int(len(ds_cases) * TRAIN_TEST_SPLIT))\n",
    "        rs = np.random.RandomState(rng_seed)\n",
    "        ds_cases = ds_cases.copy()\n",
    "        rs.shuffle(ds_cases)\n",
    "        test_cases.extend(ds_cases[:n_test].tolist())\n",
    "        train_cases.extend(ds_cases[n_test:].tolist())\n",
    "\n",
    "    dataset_json = {\n",
    "        \"name\": TASK_NAME,\n",
    "        \"description\": \"T1c/T1GD enhancing tumor segmentation across multiple datasets\",\n",
    "        \"reference\": \"DHAI\",\n",
    "        \"licence\": \"private\",\n",
    "        \"release\": \"0.1\",\n",
    "        \"tensorImageSize\": \"3D\",\n",
    "        \"modality\": {\"0\": \"T1gd\"},\n",
    "        \"labels\": {\"background\": 0, \"ET\": 1},\n",
    "        \"numTraining\": len(training_entries),\n",
    "        \"file_ending\": \".nii.gz\",\n",
    "        \"training\": training_entries,\n",
    "        \"test\": []\n",
    "    }\n",
    "\n",
    "    if WRITE_FILES:\n",
    "        with open(TASK_DIR / \"dataset.json\", \"w\") as f:\n",
    "            json.dump(dataset_json, f, indent=2)\n",
    "        print(f\"Wrote dataset.json with {len(training_entries)} samples to {TASK_DIR}\")\n",
    "\n",
    "    print(\"Totals:\", len(training_entries), \"trainable samples\")\n",
    "    by_ds = {}\n",
    "    for ds_unique in sorted(set(r[\"dataset\"] for r in meta_rows)):\n",
    "        by_ds[ds_unique] = sum(1 for e in training_entries if e[\"image\"].startswith(f\"./imagesTr/{ds_unique}__\"))\n",
    "    print(\"By dataset:\", by_ds)\n",
    "    return dataset_json, train_cases, test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02bb05d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] MET__BraTS-MET-00086-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00089-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00090-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00098-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00100-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00106-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00107-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00108-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00109-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00111-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00113-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00119-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00120-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00124-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00136-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00137-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00139-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00143-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00148-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00149-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00152-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00158-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00162-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00167-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00169-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00174-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00185-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00190-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00192-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00196-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00197-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00204-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00205-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00207-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00209-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00210-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00223-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00225-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00226-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00239-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00242-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00243-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00249-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00252-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00256-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00258-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00269-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00272-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00275-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00281-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00282-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00283-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00285-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00291-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00295-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00298-000: binary mask has zero positive voxels\n",
      "[SKIP] MET__BraTS-MET-00309-000: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0002_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0005_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0008_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0008_03: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0010_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0011_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0012_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0016_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0017_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0017_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0018_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0018_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0019_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0019_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0049_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0055_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0066_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0081_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0085_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0091_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0119_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0119_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0123_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0123_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0123_03: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0134_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0135_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0135_03: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0140_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0147_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0147_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0150_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0163_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0163_03: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0164_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0165_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0167_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0172_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0172_02: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0174_01: binary mask has zero positive voxels\n",
      "[SKIP] UCSD__UCSD-PTGBM-0174_02: binary mask has zero positive voxels\n",
      "[SKIP] MU__PatientID_0029_Timepoint_4: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0030_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0044_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0053_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0066_Timepoint_5: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0066_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0074_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0089_Timepoint_4: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0156_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0156_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0156_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0191_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0192_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0192_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0192_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0195_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0195_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0195_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0196_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0196_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0197_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0197_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0202_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0202_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0211_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0213_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0220_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0236_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0236_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0239_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0240_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0240_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0242_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0242_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0244_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0255_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0255_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0260_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0260_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0261_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0264_Timepoint_2: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0268_Timepoint_1: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0268_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0268_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0271_Timepoint_3: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0271_Timepoint_6: no voxels for enhancing labels [3]\n",
      "[SKIP] MU__PatientID_0275_Timepoint_3: no voxels for enhancing labels [3]\n",
      "Totals: 1838 trainable samples\n",
      "By dataset: {'BCBM': 264, 'BRATSAfrica': 95, 'MET': 143, 'MU': 546, 'UCSD': 143, 'UCSF': 500, 'UPENN': 147}\n",
      "DRY RUN (no writes): True\n",
      "Example entry: {'image': './imagesTr/MET__BraTS-MET-00096-000_0000.nii.gz', 'label': './labelsTr/MET__BraTS-MET-00096-000.nii.gz'}\n"
     ]
    }
   ],
   "source": [
    "dataset_json, train_cases, test_cases = stage_and_generate(items)\n",
    "\n",
    "print(\"DRY RUN (no writes):\", not WRITE_FILES)\n",
    "print(\"Example entry:\", dataset_json[\"training\"][0] if dataset_json[\"training\"] else \"No entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "669025c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'description', 'reference', 'licence', 'release', 'tensorImageSize', 'modality', 'labels', 'numTraining', 'file_ending', 'training', 'test'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ddfc902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': './imagesTr/BRATSAfrica__BraTS-SSA-00140-000_0000.nii.gz',\n",
       " 'label': './labelsTr/BRATSAfrica__BraTS-SSA-00140-000.nii.gz'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_json['training'][1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523fd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b17db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d288e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c1483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
